{
    "sourceFile": "training/trainer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 15,
            "patches": [
                {
                    "date": 1768337537716,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1768338384944,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,15 +1,9 @@\n import numpy as np\r\n \r\n \r\n class Trainer:\r\n-    \"\"\"\r\n-    Trainer class ŸÖÿ≠ÿ≥ŸëŸÜ ŸÖÿπ:\r\n-    - Early Stopping ŸÑŸÖŸÜÿπ overfitting\r\n-    - L2 Regularization\r\n-    - Learning Rate Scheduling\r\n-    - Model Checkpointing\r\n-    \"\"\"\r\n+\r\n     \r\n     def __init__(self, network, optimizer, weight_decay=0.0):\r\n         \"\"\"\r\n         Args:\r\n"
                },
                {
                    "date": 1768338467928,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,14 +4,8 @@\n class Trainer:\r\n \r\n     \r\n     def __init__(self, network, optimizer, weight_decay=0.0):\r\n-        \"\"\"\r\n-        Args:\r\n-            network: NeuralNetwork object\r\n-            optimizer: Optimizer object (SGD, Adam, etc.)\r\n-            weight_decay: L2 regularization coefficient (default: 0.0)\r\n-        \"\"\"\r\n         self.network = network\r\n         self.optimizer = optimizer\r\n         self.weight_decay = weight_decay\r\n         \r\n"
                },
                {
                    "date": 1768338474017,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,17 +1,13 @@\n import numpy as np\r\n \r\n \r\n class Trainer:\r\n-\r\n-    \r\n     def __init__(self, network, optimizer, weight_decay=0.0):\r\n         self.network = network\r\n         self.optimizer = optimizer\r\n         self.weight_decay = weight_decay\r\n-        \r\n-        # Training history\r\n-        self.train_loss_list = []\r\n+                self.train_loss_list = []\r\n         self.train_acc_list = []\r\n         self.test_acc_list = []\r\n         \r\n         # Early stopping\r\n"
                },
                {
                    "date": 1768338483934,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,12 +14,8 @@\n         self.best_test_acc = 0.0\r\n         self.best_params = None\r\n         \r\n     def train_step(self, x_batch, y_batch):\r\n-        \"\"\"\r\n-        ÿÆÿ∑Ÿàÿ© ÿ™ÿØÿ±Ÿäÿ® Ÿàÿßÿ≠ÿØÿ© ŸÖÿπ L2 regularization\r\n-        \"\"\"\r\n-        # Forward + backward\r\n         loss = self.network.forward(x_batch, y_batch)\r\n         self.network.backward()\r\n         \r\n         # Get params and grads\r\n"
                },
                {
                    "date": 1768338493106,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,27 +5,25 @@\n     def __init__(self, network, optimizer, weight_decay=0.0):\r\n         self.network = network\r\n         self.optimizer = optimizer\r\n         self.weight_decay = weight_decay\r\n-                self.train_loss_list = []\r\n+        \r\n+        self.train_loss_list = []\r\n         self.train_acc_list = []\r\n         self.test_acc_list = []\r\n         \r\n-        # Early stopping\r\n         self.best_test_acc = 0.0\r\n         self.best_params = None\r\n         \r\n     def train_step(self, x_batch, y_batch):\r\n         loss = self.network.forward(x_batch, y_batch)\r\n         self.network.backward()\r\n         \r\n-        # Get params and grads\r\n         params_list, grads_list = self.network.get_params_and_grads()\r\n         \r\n-        # Apply L2 regularization to gradients\r\n         if self.weight_decay > 0:\r\n             for params, grads in zip(params_list, grads_list):\r\n-                if 'W' in grads:  # Apply only to weights, not biases\r\n+                if 'W' in grads:  \r\n                     grads['W'] += self.weight_decay * params['W']\r\n         \r\n         # Update parameters\r\n         for params, grads in zip(params_list, grads_list):\r\n"
                },
                {
                    "date": 1768338498313,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,159 @@\n+import numpy as np\r\n+\r\n+\r\n+class Trainer:\r\n+    def __init__(self, network, optimizer, weight_decay=0.0):\r\n+        self.network = network\r\n+        self.optimizer = optimizer\r\n+        self.weight_decay = weight_decay\r\n+        \r\n+        self.train_loss_list = []\r\n+        self.train_acc_list = []\r\n+        self.test_acc_list = []\r\n+        \r\n+        self.best_test_acc = 0.0\r\n+        self.best_params = None\r\n+        \r\n+    def train_step(self, x_batch, y_batch):\r\n+        loss = self.network.forward(x_batch, y_batch)\r\n+        self.network.backward()\r\n+        \r\n+        params_list, grads_list = self.network.get_params_and_grads()\r\n+        \r\n+        if self.weight_decay > 0:\r\n+            for params, grads in zip(params_list, grads_list):\r\n+                if 'W' in grads:  \r\n+                    grads['W'] += self.weight_decay * params['W']\r\n+        \r\n+        for params, grads in zip(params_list, grads_list):\r\n+            self.optimizer.update(params, grads)\r\n+        \r\n+        if self.weight_decay > 0:\r\n+            l2_loss = 0\r\n+            for params in params_list:\r\n+                if 'W' in params:\r\n+                    l2_loss += np.sum(params['W'] ** 2)\r\n+            loss += 0.5 * self.weight_decay * l2_loss\r\n+        \r\n+        return loss\r\n+    \r\n+    def fit(self, x_train, y_train, x_test=None, y_test=None, \r\n+            epochs=10, batch_size=32, verbose=True, \r\n+            early_stopping=False, patience=10, \r\n+            lr_decay=False, decay_rate=0.95, decay_every=10):\r\n+        \"\"\"\r\n+        ÿ™ÿØÿ±Ÿäÿ® ÿßŸÑÿ¥ÿ®ŸÉÿ© ŸÖÿπ ŸÖŸäÿ≤ÿßÿ™ ŸÖÿ™ŸÇÿØŸÖÿ©\r\n+        \r\n+        Args:\r\n+            x_train, y_train: Training data\r\n+            x_test, y_test: Test data (optional)\r\n+            epochs: ÿπÿØÿØ ÿßŸÑŸÄ epochs\r\n+            batch_size: ÿ≠ÿ¨ŸÖ ÿßŸÑŸÄ batch\r\n+            verbose: ÿ∑ÿ®ÿßÿπÿ© ÿßŸÑÿ™ŸÇÿØŸÖ\r\n+            early_stopping: ÿ™ŸÅÿπŸäŸÑ early stopping\r\n+            patience: ÿπÿØÿØ epochs ŸÑŸÑÿßŸÜÿ™ÿ∏ÿßÿ± ŸÇÿ®ŸÑ ÿßŸÑÿ™ŸàŸÇŸÅ\r\n+            lr_decay: ÿ™ŸÅÿπŸäŸÑ learning rate decay\r\n+            decay_rate: ŸÖÿπÿØŸÑ ÿßŸÑÿßŸÜÿÆŸÅÿßÿ∂\r\n+            decay_every: ŸÉŸÑ ŸÉŸÖ epoch ŸäŸÜÿÆŸÅÿ∂ ÿßŸÑŸÄ LR\r\n+        \"\"\"\r\n+        train_size = x_train.shape[0]\r\n+        iter_per_epoch = max(train_size // batch_size, 1)\r\n+        \r\n+        # Early stopping\r\n+        patience_counter = 0\r\n+        self.best_test_acc = 0.0\r\n+        \r\n+        for epoch in range(epochs):\r\n+            # Set training mode\r\n+            self.network.set_train_mode(True)\r\n+            \r\n+            # Shuffle data\r\n+            idx = np.random.permutation(train_size)\r\n+            x_shuffled = x_train[idx]\r\n+            y_shuffled = y_train[idx]\r\n+            \r\n+            # Training loop\r\n+            epoch_loss = 0\r\n+            for i in range(iter_per_epoch):\r\n+                start = i * batch_size\r\n+                end = start + batch_size\r\n+                x_batch = x_shuffled[start:end]\r\n+                y_batch = y_shuffled[start:end]\r\n+                \r\n+                loss = self.train_step(x_batch, y_batch)\r\n+                epoch_loss += loss\r\n+            \r\n+            avg_loss = epoch_loss / iter_per_epoch\r\n+            self.train_loss_list.append(avg_loss)\r\n+            \r\n+            # Evaluation\r\n+            self.network.set_train_mode(False)\r\n+            train_acc = self.network.accuracy(x_train, y_train)\r\n+            self.train_acc_list.append(train_acc)\r\n+            \r\n+            # Test accuracy\r\n+            if x_test is not None and y_test is not None:\r\n+                test_acc = self.network.accuracy(x_test, y_test)\r\n+                self.test_acc_list.append(test_acc)\r\n+                \r\n+                # Early stopping check\r\n+                if early_stopping:\r\n+                    if test_acc > self.best_test_acc:\r\n+                        self.best_test_acc = test_acc\r\n+                        patience_counter = 0\r\n+                        # Save best params\r\n+                        self.best_params = self._save_params()\r\n+                    else:\r\n+                        patience_counter += 1\r\n+                    \r\n+                    if patience_counter >= patience:\r\n+                        if verbose:\r\n+                            print(f\"\\n‚ö†Ô∏è Early stopping at epoch {epoch + 1}\")\r\n+                            print(f\"Best test accuracy: {self.best_test_acc:.4f}\")\r\n+                        # Restore best params\r\n+                        self._restore_params(self.best_params)\r\n+                        break\r\n+                \r\n+                if verbose:\r\n+                    gap = train_acc - test_acc\r\n+                    gap_indicator = \"üî¥\" if gap > 0.05 else \"‚úÖ\"\r\n+                    print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n+                          f\"Loss: {avg_loss:.4f} - \"\r\n+                          f\"Train Acc: {train_acc:.4f} - \"\r\n+                          f\"Test Acc: {test_acc:.4f} - \"\r\n+                          f\"Gap: {gap:.4f} {gap_indicator}\")\r\n+            else:\r\n+                if verbose:\r\n+                    print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n+                          f\"Loss: {avg_loss:.4f} - \"\r\n+                          f\"Train Acc: {train_acc:.4f}\")\r\n+            \r\n+            # Learning rate decay\r\n+            if lr_decay and (epoch + 1) % decay_every == 0:\r\n+                if hasattr(self.optimizer, 'lr'):\r\n+                    self.optimizer.lr *= decay_rate\r\n+                    if verbose:\r\n+                        print(f\"üìâ Learning rate decayed to {self.optimizer.lr:.6f}\")\r\n+    \r\n+    def _save_params(self):\r\n+        \"\"\"ÿ≠ŸÅÿ∏ parameters ÿßŸÑÿ¥ÿ®ŸÉÿ©\"\"\"\r\n+        saved = []\r\n+        for layer in self.network.layers:\r\n+            if hasattr(layer, 'params') and layer.params:\r\n+                layer_params = {}\r\n+                for key, val in layer.params.items():\r\n+                    layer_params[key] = val.copy()\r\n+                saved.append(layer_params)\r\n+            else:\r\n+                saved.append(None)\r\n+        return saved\r\n+    \r\n+    def _restore_params(self, saved_params):\r\n+        \"\"\"ÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ parameters ÿßŸÑŸÖÿ≠ŸÅŸàÿ∏ÿ©\"\"\"\r\n+        for layer, params in zip(self.network.layers, saved_params):\r\n+            if params is not None:\r\n+                for key, val in params.items():\r\n+                    layer.params[key] = val.copy()\r\n+                    # Update layer attributes\r\n+                    if hasattr(layer, key):\r\n+                        setattr(layer, key, val.copy())\r\n"
                },
                {
                    "date": 1768338505687,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -40,23 +40,9 @@\n     def fit(self, x_train, y_train, x_test=None, y_test=None, \r\n             epochs=10, batch_size=32, verbose=True, \r\n             early_stopping=False, patience=10, \r\n             lr_decay=False, decay_rate=0.95, decay_every=10):\r\n-        \"\"\"\r\n-        ÿ™ÿØÿ±Ÿäÿ® ÿßŸÑÿ¥ÿ®ŸÉÿ© ŸÖÿπ ŸÖŸäÿ≤ÿßÿ™ ŸÖÿ™ŸÇÿØŸÖÿ©\r\n-        \r\n-        Args:\r\n-            x_train, y_train: Training data\r\n-            x_test, y_test: Test data (optional)\r\n-            epochs: ÿπÿØÿØ ÿßŸÑŸÄ epochs\r\n-            batch_size: ÿ≠ÿ¨ŸÖ ÿßŸÑŸÄ batch\r\n-            verbose: ÿ∑ÿ®ÿßÿπÿ© ÿßŸÑÿ™ŸÇÿØŸÖ\r\n-            early_stopping: ÿ™ŸÅÿπŸäŸÑ early stopping\r\n-            patience: ÿπÿØÿØ epochs ŸÑŸÑÿßŸÜÿ™ÿ∏ÿßÿ± ŸÇÿ®ŸÑ ÿßŸÑÿ™ŸàŸÇŸÅ\r\n-            lr_decay: ÿ™ŸÅÿπŸäŸÑ learning rate decay\r\n-            decay_rate: ŸÖÿπÿØŸÑ ÿßŸÑÿßŸÜÿÆŸÅÿßÿ∂\r\n-            decay_every: ŸÉŸÑ ŸÉŸÖ epoch ŸäŸÜÿÆŸÅÿ∂ ÿßŸÑŸÄ LR\r\n-        \"\"\"\r\n+       \r\n         train_size = x_train.shape[0]\r\n         iter_per_epoch = max(train_size // batch_size, 1)\r\n         \r\n         # Early stopping\r\n"
                },
                {
                    "date": 1768338512594,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -40,18 +40,15 @@\n     def fit(self, x_train, y_train, x_test=None, y_test=None, \r\n             epochs=10, batch_size=32, verbose=True, \r\n             early_stopping=False, patience=10, \r\n             lr_decay=False, decay_rate=0.95, decay_every=10):\r\n-       \r\n         train_size = x_train.shape[0]\r\n         iter_per_epoch = max(train_size // batch_size, 1)\r\n         \r\n-        # Early stopping\r\n         patience_counter = 0\r\n         self.best_test_acc = 0.0\r\n         \r\n         for epoch in range(epochs):\r\n-            # Set training mode\r\n             self.network.set_train_mode(True)\r\n             \r\n             # Shuffle data\r\n             idx = np.random.permutation(train_size)\r\n@@ -142,165 +139,4 @@\n                     layer.params[key] = val.copy()\r\n                     # Update layer attributes\r\n                     if hasattr(layer, key):\r\n                         setattr(layer, key, val.copy())\r\n-import numpy as np\r\n-\r\n-\r\n-class Trainer:\r\n-    def __init__(self, network, optimizer, weight_decay=0.0):\r\n-        self.network = network\r\n-        self.optimizer = optimizer\r\n-        self.weight_decay = weight_decay\r\n-        \r\n-        self.train_loss_list = []\r\n-        self.train_acc_list = []\r\n-        self.test_acc_list = []\r\n-        \r\n-        self.best_test_acc = 0.0\r\n-        self.best_params = None\r\n-        \r\n-    def train_step(self, x_batch, y_batch):\r\n-        loss = self.network.forward(x_batch, y_batch)\r\n-        self.network.backward()\r\n-        \r\n-        params_list, grads_list = self.network.get_params_and_grads()\r\n-        \r\n-        if self.weight_decay > 0:\r\n-            for params, grads in zip(params_list, grads_list):\r\n-                if 'W' in grads:  \r\n-                    grads['W'] += self.weight_decay * params['W']\r\n-        \r\n-        # Update parameters\r\n-        for params, grads in zip(params_list, grads_list):\r\n-            self.optimizer.update(params, grads)\r\n-        \r\n-        # Add L2 penalty to loss\r\n-        if self.weight_decay > 0:\r\n-            l2_loss = 0\r\n-            for params in params_list:\r\n-                if 'W' in params:\r\n-                    l2_loss += np.sum(params['W'] ** 2)\r\n-            loss += 0.5 * self.weight_decay * l2_loss\r\n-        \r\n-        return loss\r\n-    \r\n-    def fit(self, x_train, y_train, x_test=None, y_test=None, \r\n-            epochs=10, batch_size=32, verbose=True, \r\n-            early_stopping=False, patience=10, \r\n-            lr_decay=False, decay_rate=0.95, decay_every=10):\r\n-        \"\"\"\r\n-        ÿ™ÿØÿ±Ÿäÿ® ÿßŸÑÿ¥ÿ®ŸÉÿ© ŸÖÿπ ŸÖŸäÿ≤ÿßÿ™ ŸÖÿ™ŸÇÿØŸÖÿ©\r\n-        \r\n-        Args:\r\n-            x_train, y_train: Training data\r\n-            x_test, y_test: Test data (optional)\r\n-            epochs: ÿπÿØÿØ ÿßŸÑŸÄ epochs\r\n-            batch_size: ÿ≠ÿ¨ŸÖ ÿßŸÑŸÄ batch\r\n-            verbose: ÿ∑ÿ®ÿßÿπÿ© ÿßŸÑÿ™ŸÇÿØŸÖ\r\n-            early_stopping: ÿ™ŸÅÿπŸäŸÑ early stopping\r\n-            patience: ÿπÿØÿØ epochs ŸÑŸÑÿßŸÜÿ™ÿ∏ÿßÿ± ŸÇÿ®ŸÑ ÿßŸÑÿ™ŸàŸÇŸÅ\r\n-            lr_decay: ÿ™ŸÅÿπŸäŸÑ learning rate decay\r\n-            decay_rate: ŸÖÿπÿØŸÑ ÿßŸÑÿßŸÜÿÆŸÅÿßÿ∂\r\n-            decay_every: ŸÉŸÑ ŸÉŸÖ epoch ŸäŸÜÿÆŸÅÿ∂ ÿßŸÑŸÄ LR\r\n-        \"\"\"\r\n-        train_size = x_train.shape[0]\r\n-        iter_per_epoch = max(train_size // batch_size, 1)\r\n-        \r\n-        # Early stopping\r\n-        patience_counter = 0\r\n-        self.best_test_acc = 0.0\r\n-        \r\n-        for epoch in range(epochs):\r\n-            # Set training mode\r\n-            self.network.set_train_mode(True)\r\n-            \r\n-            # Shuffle data\r\n-            idx = np.random.permutation(train_size)\r\n-            x_shuffled = x_train[idx]\r\n-            y_shuffled = y_train[idx]\r\n-            \r\n-            # Training loop\r\n-            epoch_loss = 0\r\n-            for i in range(iter_per_epoch):\r\n-                start = i * batch_size\r\n-                end = start + batch_size\r\n-                x_batch = x_shuffled[start:end]\r\n-                y_batch = y_shuffled[start:end]\r\n-                \r\n-                loss = self.train_step(x_batch, y_batch)\r\n-                epoch_loss += loss\r\n-            \r\n-            avg_loss = epoch_loss / iter_per_epoch\r\n-            self.train_loss_list.append(avg_loss)\r\n-            \r\n-            # Evaluation\r\n-            self.network.set_train_mode(False)\r\n-            train_acc = self.network.accuracy(x_train, y_train)\r\n-            self.train_acc_list.append(train_acc)\r\n-            \r\n-            # Test accuracy\r\n-            if x_test is not None and y_test is not None:\r\n-                test_acc = self.network.accuracy(x_test, y_test)\r\n-                self.test_acc_list.append(test_acc)\r\n-                \r\n-                # Early stopping check\r\n-                if early_stopping:\r\n-                    if test_acc > self.best_test_acc:\r\n-                        self.best_test_acc = test_acc\r\n-                        patience_counter = 0\r\n-                        # Save best params\r\n-                        self.best_params = self._save_params()\r\n-                    else:\r\n-                        patience_counter += 1\r\n-                    \r\n-                    if patience_counter >= patience:\r\n-                        if verbose:\r\n-                            print(f\"\\n‚ö†Ô∏è Early stopping at epoch {epoch + 1}\")\r\n-                            print(f\"Best test accuracy: {self.best_test_acc:.4f}\")\r\n-                        # Restore best params\r\n-                        self._restore_params(self.best_params)\r\n-                        break\r\n-                \r\n-                if verbose:\r\n-                    gap = train_acc - test_acc\r\n-                    gap_indicator = \"üî¥\" if gap > 0.05 else \"‚úÖ\"\r\n-                    print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n-                          f\"Loss: {avg_loss:.4f} - \"\r\n-                          f\"Train Acc: {train_acc:.4f} - \"\r\n-                          f\"Test Acc: {test_acc:.4f} - \"\r\n-                          f\"Gap: {gap:.4f} {gap_indicator}\")\r\n-            else:\r\n-                if verbose:\r\n-                    print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n-                          f\"Loss: {avg_loss:.4f} - \"\r\n-                          f\"Train Acc: {train_acc:.4f}\")\r\n-            \r\n-            # Learning rate decay\r\n-            if lr_decay and (epoch + 1) % decay_every == 0:\r\n-                if hasattr(self.optimizer, 'lr'):\r\n-                    self.optimizer.lr *= decay_rate\r\n-                    if verbose:\r\n-                        print(f\"üìâ Learning rate decayed to {self.optimizer.lr:.6f}\")\r\n-    \r\n-    def _save_params(self):\r\n-        \"\"\"ÿ≠ŸÅÿ∏ parameters ÿßŸÑÿ¥ÿ®ŸÉÿ©\"\"\"\r\n-        saved = []\r\n-        for layer in self.network.layers:\r\n-            if hasattr(layer, 'params') and layer.params:\r\n-                layer_params = {}\r\n-                for key, val in layer.params.items():\r\n-                    layer_params[key] = val.copy()\r\n-                saved.append(layer_params)\r\n-            else:\r\n-                saved.append(None)\r\n-        return saved\r\n-    \r\n-    def _restore_params(self, saved_params):\r\n-        \"\"\"ÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ parameters ÿßŸÑŸÖÿ≠ŸÅŸàÿ∏ÿ©\"\"\"\r\n-        for layer, params in zip(self.network.layers, saved_params):\r\n-            if params is not None:\r\n-                for key, val in params.items():\r\n-                    layer.params[key] = val.copy()\r\n-                    # Update layer attributes\r\n-                    if hasattr(layer, key):\r\n-                        setattr(layer, key, val.copy())\r\n"
                },
                {
                    "date": 1768338518218,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -49,14 +49,12 @@\n         \r\n         for epoch in range(epochs):\r\n             self.network.set_train_mode(True)\r\n             \r\n-            # Shuffle data\r\n             idx = np.random.permutation(train_size)\r\n             x_shuffled = x_train[idx]\r\n             y_shuffled = y_train[idx]\r\n             \r\n-            # Training loop\r\n             epoch_loss = 0\r\n             for i in range(iter_per_epoch):\r\n                 start = i * batch_size\r\n                 end = start + batch_size\r\n@@ -68,9 +66,8 @@\n             \r\n             avg_loss = epoch_loss / iter_per_epoch\r\n             self.train_loss_list.append(avg_loss)\r\n             \r\n-            # Evaluation\r\n             self.network.set_train_mode(False)\r\n             train_acc = self.network.accuracy(x_train, y_train)\r\n             self.train_acc_list.append(train_acc)\r\n             \r\n"
                },
                {
                    "date": 1768338531640,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -70,14 +70,12 @@\n             self.network.set_train_mode(False)\r\n             train_acc = self.network.accuracy(x_train, y_train)\r\n             self.train_acc_list.append(train_acc)\r\n             \r\n-            # Test accuracy\r\n             if x_test is not None and y_test is not None:\r\n                 test_acc = self.network.accuracy(x_test, y_test)\r\n                 self.test_acc_list.append(test_acc)\r\n                 \r\n-                # Early stopping check\r\n                 if early_stopping:\r\n                     if test_acc > self.best_test_acc:\r\n                         self.best_test_acc = test_acc\r\n                         patience_counter = 0\r\n@@ -87,9 +85,9 @@\n                         patience_counter += 1\r\n                     \r\n                     if patience_counter >= patience:\r\n                         if verbose:\r\n-                            print(f\"\\n‚ö†Ô∏è Early stopping at epoch {epoch + 1}\")\r\n+                            print(f\"\\nEarly stopping at epoch {epoch + 1}\")\r\n                             print(f\"Best test accuracy: {self.best_test_acc:.4f}\")\r\n                         # Restore best params\r\n                         self._restore_params(self.best_params)\r\n                         break\r\n"
                },
                {
                    "date": 1768338546829,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -98,9 +98,9 @@\n                     print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n                           f\"Loss: {avg_loss:.4f} - \"\r\n                           f\"Train Acc: {train_acc:.4f} - \"\r\n                           f\"Test Acc: {test_acc:.4f} - \"\r\n-                          f\"Gap: {gap:.4f} {gap_indicator}\")\r\n+                          f\"Gap: {gap:.4f}\")\r\n             else:\r\n                 if verbose:\r\n                     print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n                           f\"Loss: {avg_loss:.4f} - \"\r\n"
                },
                {
                    "date": 1768338554111,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,17 +85,16 @@\n                         patience_counter += 1\r\n                     \r\n                     if patience_counter >= patience:\r\n                         if verbose:\r\n-                            print(f\"\\nEarly stopping at epoch {epoch + 1}\")\r\n+                            print(f\"\\n Early stopping at epoch {epoch + 1}\")\r\n                             print(f\"Best test accuracy: {self.best_test_acc:.4f}\")\r\n                         # Restore best params\r\n                         self._restore_params(self.best_params)\r\n                         break\r\n                 \r\n                 if verbose:\r\n                     gap = train_acc - test_acc\r\n-                    gap_indicator = \"üî¥\" if gap > 0.05 else \"‚úÖ\"\r\n                     print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n                           f\"Loss: {avg_loss:.4f} - \"\r\n                           f\"Train Acc: {train_acc:.4f} - \"\r\n                           f\"Test Acc: {test_acc:.4f} - \"\r\n@@ -105,9 +104,8 @@\n                     print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n                           f\"Loss: {avg_loss:.4f} - \"\r\n                           f\"Train Acc: {train_acc:.4f}\")\r\n             \r\n-            # Learning rate decay\r\n             if lr_decay and (epoch + 1) % decay_every == 0:\r\n                 if hasattr(self.optimizer, 'lr'):\r\n                     self.optimizer.lr *= decay_rate\r\n                     if verbose:\r\n"
                },
                {
                    "date": 1768338561236,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,134 @@\n+import numpy as np\r\n+\r\n+\r\n+class Trainer:\r\n+    def __init__(self, network, optimizer, weight_decay=0.0):\r\n+        self.network = network\r\n+        self.optimizer = optimizer\r\n+        self.weight_decay = weight_decay\r\n+        \r\n+        self.train_loss_list = []\r\n+        self.train_acc_list = []\r\n+        self.test_acc_list = []\r\n+        \r\n+        self.best_test_acc = 0.0\r\n+        self.best_params = None\r\n+        \r\n+    def train_step(self, x_batch, y_batch):\r\n+        loss = self.network.forward(x_batch, y_batch)\r\n+        self.network.backward()\r\n+        \r\n+        params_list, grads_list = self.network.get_params_and_grads()\r\n+        \r\n+        if self.weight_decay > 0:\r\n+            for params, grads in zip(params_list, grads_list):\r\n+                if 'W' in grads:  \r\n+                    grads['W'] += self.weight_decay * params['W']\r\n+        \r\n+        for params, grads in zip(params_list, grads_list):\r\n+            self.optimizer.update(params, grads)\r\n+        \r\n+        if self.weight_decay > 0:\r\n+            l2_loss = 0\r\n+            for params in params_list:\r\n+                if 'W' in params:\r\n+                    l2_loss += np.sum(params['W'] ** 2)\r\n+            loss += 0.5 * self.weight_decay * l2_loss\r\n+        \r\n+        return loss\r\n+    \r\n+    def fit(self, x_train, y_train, x_test=None, y_test=None, \r\n+            epochs=10, batch_size=32, verbose=True, \r\n+            early_stopping=False, patience=10, \r\n+            lr_decay=False, decay_rate=0.95, decay_every=10):\r\n+        train_size = x_train.shape[0]\r\n+        iter_per_epoch = max(train_size // batch_size, 1)\r\n+        \r\n+        patience_counter = 0\r\n+        self.best_test_acc = 0.0\r\n+        \r\n+        for epoch in range(epochs):\r\n+            self.network.set_train_mode(True)\r\n+            \r\n+            idx = np.random.permutation(train_size)\r\n+            x_shuffled = x_train[idx]\r\n+            y_shuffled = y_train[idx]\r\n+            \r\n+            epoch_loss = 0\r\n+            for i in range(iter_per_epoch):\r\n+                start = i * batch_size\r\n+                end = start + batch_size\r\n+                x_batch = x_shuffled[start:end]\r\n+                y_batch = y_shuffled[start:end]\r\n+                \r\n+                loss = self.train_step(x_batch, y_batch)\r\n+                epoch_loss += loss\r\n+            \r\n+            avg_loss = epoch_loss / iter_per_epoch\r\n+            self.train_loss_list.append(avg_loss)\r\n+            \r\n+            self.network.set_train_mode(False)\r\n+            train_acc = self.network.accuracy(x_train, y_train)\r\n+            self.train_acc_list.append(train_acc)\r\n+            \r\n+            if x_test is not None and y_test is not None:\r\n+                test_acc = self.network.accuracy(x_test, y_test)\r\n+                self.test_acc_list.append(test_acc)\r\n+                \r\n+                if early_stopping:\r\n+                    if test_acc > self.best_test_acc:\r\n+                        self.best_test_acc = test_acc\r\n+                        patience_counter = 0\r\n+                        # Save best params\r\n+                        self.best_params = self._save_params()\r\n+                    else:\r\n+                        patience_counter += 1\r\n+                    \r\n+                    if patience_counter >= patience:\r\n+                        if verbose:\r\n+                            print(f\"\\n Early stopping at epoch {epoch + 1}\")\r\n+                            print(f\"Best test accuracy: {self.best_test_acc:.4f}\")\r\n+                        # Restore best params\r\n+                        self._restore_params(self.best_params)\r\n+                        break\r\n+                \r\n+                if verbose:\r\n+                    gap = train_acc - test_acc\r\n+                    print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n+                          f\"Loss: {avg_loss:.4f} - \"\r\n+                          f\"Train Acc: {train_acc:.4f} - \"\r\n+                          f\"Test Acc: {test_acc:.4f} - \"\r\n+                          f\"Gap: {gap:.4f}\")\r\n+            else:\r\n+                if verbose:\r\n+                    print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n+                          f\"Loss: {avg_loss:.4f} - \"\r\n+                          f\"Train Acc: {train_acc:.4f}\")\r\n+            \r\n+            if lr_decay and (epoch + 1) % decay_every == 0:\r\n+                if hasattr(self.optimizer, 'lr'):\r\n+                    self.optimizer.lr *= decay_rate\r\n+                    if verbose:\r\n+                        print(f\"Learning rate decayed to {self.optimizer.lr:.6f}\")\r\n+    \r\n+    def _save_params(self):\r\n+        saved = []\r\n+        for layer in self.network.layers:\r\n+            if hasattr(layer, 'params') and layer.params:\r\n+                layer_params = {}\r\n+                for key, val in layer.params.items():\r\n+                    layer_params[key] = val.copy()\r\n+                saved.append(layer_params)\r\n+            else:\r\n+                saved.append(None)\r\n+        return saved\r\n+    \r\n+    def _restore_params(self, saved_params):\r\n+        \"\"\"ÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ parameters ÿßŸÑŸÖÿ≠ŸÅŸàÿ∏ÿ©\"\"\"\r\n+        for layer, params in zip(self.network.layers, saved_params):\r\n+            if params is not None:\r\n+                for key, val in params.items():\r\n+                    layer.params[key] = val.copy()\r\n+                    # Update layer attributes\r\n+                    if hasattr(layer, key):\r\n+                        setattr(layer, key, val.copy())\r\n"
                },
                {
                    "date": 1768338567206,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,132 @@\n+import numpy as np\r\n+\r\n+\r\n+class Trainer:\r\n+    def __init__(self, network, optimizer, weight_decay=0.0):\r\n+        self.network = network\r\n+        self.optimizer = optimizer\r\n+        self.weight_decay = weight_decay\r\n+        \r\n+        self.train_loss_list = []\r\n+        self.train_acc_list = []\r\n+        self.test_acc_list = []\r\n+        \r\n+        self.best_test_acc = 0.0\r\n+        self.best_params = None\r\n+        \r\n+    def train_step(self, x_batch, y_batch):\r\n+        loss = self.network.forward(x_batch, y_batch)\r\n+        self.network.backward()\r\n+        \r\n+        params_list, grads_list = self.network.get_params_and_grads()\r\n+        \r\n+        if self.weight_decay > 0:\r\n+            for params, grads in zip(params_list, grads_list):\r\n+                if 'W' in grads:  \r\n+                    grads['W'] += self.weight_decay * params['W']\r\n+        \r\n+        for params, grads in zip(params_list, grads_list):\r\n+            self.optimizer.update(params, grads)\r\n+        \r\n+        if self.weight_decay > 0:\r\n+            l2_loss = 0\r\n+            for params in params_list:\r\n+                if 'W' in params:\r\n+                    l2_loss += np.sum(params['W'] ** 2)\r\n+            loss += 0.5 * self.weight_decay * l2_loss\r\n+        \r\n+        return loss\r\n+    \r\n+    def fit(self, x_train, y_train, x_test=None, y_test=None, \r\n+            epochs=10, batch_size=32, verbose=True, \r\n+            early_stopping=False, patience=10, \r\n+            lr_decay=False, decay_rate=0.95, decay_every=10):\r\n+        train_size = x_train.shape[0]\r\n+        iter_per_epoch = max(train_size // batch_size, 1)\r\n+        \r\n+        patience_counter = 0\r\n+        self.best_test_acc = 0.0\r\n+        \r\n+        for epoch in range(epochs):\r\n+            self.network.set_train_mode(True)\r\n+            \r\n+            idx = np.random.permutation(train_size)\r\n+            x_shuffled = x_train[idx]\r\n+            y_shuffled = y_train[idx]\r\n+            \r\n+            epoch_loss = 0\r\n+            for i in range(iter_per_epoch):\r\n+                start = i * batch_size\r\n+                end = start + batch_size\r\n+                x_batch = x_shuffled[start:end]\r\n+                y_batch = y_shuffled[start:end]\r\n+                \r\n+                loss = self.train_step(x_batch, y_batch)\r\n+                epoch_loss += loss\r\n+            \r\n+            avg_loss = epoch_loss / iter_per_epoch\r\n+            self.train_loss_list.append(avg_loss)\r\n+            \r\n+            self.network.set_train_mode(False)\r\n+            train_acc = self.network.accuracy(x_train, y_train)\r\n+            self.train_acc_list.append(train_acc)\r\n+            \r\n+            if x_test is not None and y_test is not None:\r\n+                test_acc = self.network.accuracy(x_test, y_test)\r\n+                self.test_acc_list.append(test_acc)\r\n+                \r\n+                if early_stopping:\r\n+                    if test_acc > self.best_test_acc:\r\n+                        self.best_test_acc = test_acc\r\n+                        patience_counter = 0\r\n+                        # Save best params\r\n+                        self.best_params = self._save_params()\r\n+                    else:\r\n+                        patience_counter += 1\r\n+                    \r\n+                    if patience_counter >= patience:\r\n+                        if verbose:\r\n+                            print(f\"\\n Early stopping at epoch {epoch + 1}\")\r\n+                            print(f\"Best test accuracy: {self.best_test_acc:.4f}\")\r\n+                        # Restore best params\r\n+                        self._restore_params(self.best_params)\r\n+                        break\r\n+                \r\n+                if verbose:\r\n+                    gap = train_acc - test_acc\r\n+                    print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n+                          f\"Loss: {avg_loss:.4f} - \"\r\n+                          f\"Train Acc: {train_acc:.4f} - \"\r\n+                          f\"Test Acc: {test_acc:.4f} - \"\r\n+                          f\"Gap: {gap:.4f}\")\r\n+            else:\r\n+                if verbose:\r\n+                    print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n+                          f\"Loss: {avg_loss:.4f} - \"\r\n+                          f\"Train Acc: {train_acc:.4f}\")\r\n+            \r\n+            if lr_decay and (epoch + 1) % decay_every == 0:\r\n+                if hasattr(self.optimizer, 'lr'):\r\n+                    self.optimizer.lr *= decay_rate\r\n+                    if verbose:\r\n+                        print(f\"Learning rate decayed to {self.optimizer.lr:.6f}\")\r\n+    \r\n+    def _save_params(self):\r\n+        saved = []\r\n+        for layer in self.network.layers:\r\n+            if hasattr(layer, 'params') and layer.params:\r\n+                layer_params = {}\r\n+                for key, val in layer.params.items():\r\n+                    layer_params[key] = val.copy()\r\n+                saved.append(layer_params)\r\n+            else:\r\n+                saved.append(None)\r\n+        return saved\r\n+    \r\n+    def _restore_params(self, saved_params):\r\n+        for layer, params in zip(self.network.layers, saved_params):\r\n+            if params is not None:\r\n+                for key, val in params.items():\r\n+                    layer.params[key] = val.copy()\r\n+                    if hasattr(layer, key):\r\n+                        setattr(layer, key, val.copy())\r\n"
                },
                {
                    "date": 1768338574888,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,94 +5,94 @@\n     def __init__(self, network, optimizer, weight_decay=0.0):\r\n         self.network = network\r\n         self.optimizer = optimizer\r\n         self.weight_decay = weight_decay\r\n-        \r\n+\r\n         self.train_loss_list = []\r\n         self.train_acc_list = []\r\n         self.test_acc_list = []\r\n-        \r\n+\r\n         self.best_test_acc = 0.0\r\n         self.best_params = None\r\n-        \r\n+\r\n     def train_step(self, x_batch, y_batch):\r\n         loss = self.network.forward(x_batch, y_batch)\r\n         self.network.backward()\r\n-        \r\n+\r\n         params_list, grads_list = self.network.get_params_and_grads()\r\n-        \r\n+\r\n         if self.weight_decay > 0:\r\n             for params, grads in zip(params_list, grads_list):\r\n-                if 'W' in grads:  \r\n+                if 'W' in grads:\r\n                     grads['W'] += self.weight_decay * params['W']\r\n-        \r\n+\r\n         for params, grads in zip(params_list, grads_list):\r\n             self.optimizer.update(params, grads)\r\n-        \r\n+\r\n         if self.weight_decay > 0:\r\n             l2_loss = 0\r\n             for params in params_list:\r\n                 if 'W' in params:\r\n                     l2_loss += np.sum(params['W'] ** 2)\r\n             loss += 0.5 * self.weight_decay * l2_loss\r\n-        \r\n+\r\n         return loss\r\n-    \r\n-    def fit(self, x_train, y_train, x_test=None, y_test=None, \r\n-            epochs=10, batch_size=32, verbose=True, \r\n-            early_stopping=False, patience=10, \r\n+\r\n+    def fit(self, x_train, y_train, x_test=None, y_test=None,\r\n+            epochs=10, batch_size=32, verbose=True,\r\n+            early_stopping=False, patience=10,\r\n             lr_decay=False, decay_rate=0.95, decay_every=10):\r\n         train_size = x_train.shape[0]\r\n         iter_per_epoch = max(train_size // batch_size, 1)\r\n-        \r\n+\r\n         patience_counter = 0\r\n         self.best_test_acc = 0.0\r\n-        \r\n+\r\n         for epoch in range(epochs):\r\n             self.network.set_train_mode(True)\r\n-            \r\n+\r\n             idx = np.random.permutation(train_size)\r\n             x_shuffled = x_train[idx]\r\n             y_shuffled = y_train[idx]\r\n-            \r\n+\r\n             epoch_loss = 0\r\n             for i in range(iter_per_epoch):\r\n                 start = i * batch_size\r\n                 end = start + batch_size\r\n                 x_batch = x_shuffled[start:end]\r\n                 y_batch = y_shuffled[start:end]\r\n-                \r\n+\r\n                 loss = self.train_step(x_batch, y_batch)\r\n                 epoch_loss += loss\r\n-            \r\n+\r\n             avg_loss = epoch_loss / iter_per_epoch\r\n             self.train_loss_list.append(avg_loss)\r\n-            \r\n+\r\n             self.network.set_train_mode(False)\r\n             train_acc = self.network.accuracy(x_train, y_train)\r\n             self.train_acc_list.append(train_acc)\r\n-            \r\n+\r\n             if x_test is not None and y_test is not None:\r\n                 test_acc = self.network.accuracy(x_test, y_test)\r\n                 self.test_acc_list.append(test_acc)\r\n-                \r\n+\r\n                 if early_stopping:\r\n                     if test_acc > self.best_test_acc:\r\n                         self.best_test_acc = test_acc\r\n                         patience_counter = 0\r\n                         # Save best params\r\n                         self.best_params = self._save_params()\r\n                     else:\r\n                         patience_counter += 1\r\n-                    \r\n+\r\n                     if patience_counter >= patience:\r\n                         if verbose:\r\n                             print(f\"\\n Early stopping at epoch {epoch + 1}\")\r\n-                            print(f\"Best test accuracy: {self.best_test_acc:.4f}\")\r\n-                        # Restore best params\r\n+                            print(\r\n+                                f\"Best test accuracy: {self.best_test_acc:.4f}\")\r\n                         self._restore_params(self.best_params)\r\n                         break\r\n-                \r\n+\r\n                 if verbose:\r\n                     gap = train_acc - test_acc\r\n                     print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n                           f\"Loss: {avg_loss:.4f} - \"\r\n@@ -103,147 +103,16 @@\n                 if verbose:\r\n                     print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n                           f\"Loss: {avg_loss:.4f} - \"\r\n                           f\"Train Acc: {train_acc:.4f}\")\r\n-            \r\n+\r\n             if lr_decay and (epoch + 1) % decay_every == 0:\r\n                 if hasattr(self.optimizer, 'lr'):\r\n                     self.optimizer.lr *= decay_rate\r\n                     if verbose:\r\n-                        print(f\"Learning rate decayed to {self.optimizer.lr:.6f}\")\r\n-    \r\n-    def _save_params(self):\r\n-        saved = []\r\n-        for layer in self.network.layers:\r\n-            if hasattr(layer, 'params') and layer.params:\r\n-                layer_params = {}\r\n-                for key, val in layer.params.items():\r\n-                    layer_params[key] = val.copy()\r\n-                saved.append(layer_params)\r\n-            else:\r\n-                saved.append(None)\r\n-        return saved\r\n-    \r\n-    def _restore_params(self, saved_params):\r\n-        for layer, params in zip(self.network.layers, saved_params):\r\n-            if params is not None:\r\n-                for key, val in params.items():\r\n-                    layer.params[key] = val.copy()\r\n-                    if hasattr(layer, key):\r\n-                        setattr(layer, key, val.copy())\r\n-import numpy as np\r\n+                        print(\r\n+                            f\"Learning rate decayed to {self.optimizer.lr:.6f}\")\r\n \r\n-\r\n-class Trainer:\r\n-    def __init__(self, network, optimizer, weight_decay=0.0):\r\n-        self.network = network\r\n-        self.optimizer = optimizer\r\n-        self.weight_decay = weight_decay\r\n-        \r\n-        self.train_loss_list = []\r\n-        self.train_acc_list = []\r\n-        self.test_acc_list = []\r\n-        \r\n-        self.best_test_acc = 0.0\r\n-        self.best_params = None\r\n-        \r\n-    def train_step(self, x_batch, y_batch):\r\n-        loss = self.network.forward(x_batch, y_batch)\r\n-        self.network.backward()\r\n-        \r\n-        params_list, grads_list = self.network.get_params_and_grads()\r\n-        \r\n-        if self.weight_decay > 0:\r\n-            for params, grads in zip(params_list, grads_list):\r\n-                if 'W' in grads:  \r\n-                    grads['W'] += self.weight_decay * params['W']\r\n-        \r\n-        for params, grads in zip(params_list, grads_list):\r\n-            self.optimizer.update(params, grads)\r\n-        \r\n-        if self.weight_decay > 0:\r\n-            l2_loss = 0\r\n-            for params in params_list:\r\n-                if 'W' in params:\r\n-                    l2_loss += np.sum(params['W'] ** 2)\r\n-            loss += 0.5 * self.weight_decay * l2_loss\r\n-        \r\n-        return loss\r\n-    \r\n-    def fit(self, x_train, y_train, x_test=None, y_test=None, \r\n-            epochs=10, batch_size=32, verbose=True, \r\n-            early_stopping=False, patience=10, \r\n-            lr_decay=False, decay_rate=0.95, decay_every=10):\r\n-        train_size = x_train.shape[0]\r\n-        iter_per_epoch = max(train_size // batch_size, 1)\r\n-        \r\n-        patience_counter = 0\r\n-        self.best_test_acc = 0.0\r\n-        \r\n-        for epoch in range(epochs):\r\n-            self.network.set_train_mode(True)\r\n-            \r\n-            idx = np.random.permutation(train_size)\r\n-            x_shuffled = x_train[idx]\r\n-            y_shuffled = y_train[idx]\r\n-            \r\n-            epoch_loss = 0\r\n-            for i in range(iter_per_epoch):\r\n-                start = i * batch_size\r\n-                end = start + batch_size\r\n-                x_batch = x_shuffled[start:end]\r\n-                y_batch = y_shuffled[start:end]\r\n-                \r\n-                loss = self.train_step(x_batch, y_batch)\r\n-                epoch_loss += loss\r\n-            \r\n-            avg_loss = epoch_loss / iter_per_epoch\r\n-            self.train_loss_list.append(avg_loss)\r\n-            \r\n-            self.network.set_train_mode(False)\r\n-            train_acc = self.network.accuracy(x_train, y_train)\r\n-            self.train_acc_list.append(train_acc)\r\n-            \r\n-            if x_test is not None and y_test is not None:\r\n-                test_acc = self.network.accuracy(x_test, y_test)\r\n-                self.test_acc_list.append(test_acc)\r\n-                \r\n-                if early_stopping:\r\n-                    if test_acc > self.best_test_acc:\r\n-                        self.best_test_acc = test_acc\r\n-                        patience_counter = 0\r\n-                        # Save best params\r\n-                        self.best_params = self._save_params()\r\n-                    else:\r\n-                        patience_counter += 1\r\n-                    \r\n-                    if patience_counter >= patience:\r\n-                        if verbose:\r\n-                            print(f\"\\n Early stopping at epoch {epoch + 1}\")\r\n-                            print(f\"Best test accuracy: {self.best_test_acc:.4f}\")\r\n-                        # Restore best params\r\n-                        self._restore_params(self.best_params)\r\n-                        break\r\n-                \r\n-                if verbose:\r\n-                    gap = train_acc - test_acc\r\n-                    print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n-                          f\"Loss: {avg_loss:.4f} - \"\r\n-                          f\"Train Acc: {train_acc:.4f} - \"\r\n-                          f\"Test Acc: {test_acc:.4f} - \"\r\n-                          f\"Gap: {gap:.4f}\")\r\n-            else:\r\n-                if verbose:\r\n-                    print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n-                          f\"Loss: {avg_loss:.4f} - \"\r\n-                          f\"Train Acc: {train_acc:.4f}\")\r\n-            \r\n-            if lr_decay and (epoch + 1) % decay_every == 0:\r\n-                if hasattr(self.optimizer, 'lr'):\r\n-                    self.optimizer.lr *= decay_rate\r\n-                    if verbose:\r\n-                        print(f\"Learning rate decayed to {self.optimizer.lr:.6f}\")\r\n-    \r\n     def _save_params(self):\r\n         saved = []\r\n         for layer in self.network.layers:\r\n             if hasattr(layer, 'params') and layer.params:\r\n@@ -253,149 +122,12 @@\n                 saved.append(layer_params)\r\n             else:\r\n                 saved.append(None)\r\n         return saved\r\n-    \r\n-    def _restore_params(self, saved_params):\r\n-        \"\"\"ÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ parameters ÿßŸÑŸÖÿ≠ŸÅŸàÿ∏ÿ©\"\"\"\r\n-        for layer, params in zip(self.network.layers, saved_params):\r\n-            if params is not None:\r\n-                for key, val in params.items():\r\n-                    layer.params[key] = val.copy()\r\n-                    # Update layer attributes\r\n-                    if hasattr(layer, key):\r\n-                        setattr(layer, key, val.copy())\r\n-import numpy as np\r\n \r\n-\r\n-class Trainer:\r\n-    def __init__(self, network, optimizer, weight_decay=0.0):\r\n-        self.network = network\r\n-        self.optimizer = optimizer\r\n-        self.weight_decay = weight_decay\r\n-        \r\n-        self.train_loss_list = []\r\n-        self.train_acc_list = []\r\n-        self.test_acc_list = []\r\n-        \r\n-        self.best_test_acc = 0.0\r\n-        self.best_params = None\r\n-        \r\n-    def train_step(self, x_batch, y_batch):\r\n-        loss = self.network.forward(x_batch, y_batch)\r\n-        self.network.backward()\r\n-        \r\n-        params_list, grads_list = self.network.get_params_and_grads()\r\n-        \r\n-        if self.weight_decay > 0:\r\n-            for params, grads in zip(params_list, grads_list):\r\n-                if 'W' in grads:  \r\n-                    grads['W'] += self.weight_decay * params['W']\r\n-        \r\n-        for params, grads in zip(params_list, grads_list):\r\n-            self.optimizer.update(params, grads)\r\n-        \r\n-        if self.weight_decay > 0:\r\n-            l2_loss = 0\r\n-            for params in params_list:\r\n-                if 'W' in params:\r\n-                    l2_loss += np.sum(params['W'] ** 2)\r\n-            loss += 0.5 * self.weight_decay * l2_loss\r\n-        \r\n-        return loss\r\n-    \r\n-    def fit(self, x_train, y_train, x_test=None, y_test=None, \r\n-            epochs=10, batch_size=32, verbose=True, \r\n-            early_stopping=False, patience=10, \r\n-            lr_decay=False, decay_rate=0.95, decay_every=10):\r\n-        train_size = x_train.shape[0]\r\n-        iter_per_epoch = max(train_size // batch_size, 1)\r\n-        \r\n-        patience_counter = 0\r\n-        self.best_test_acc = 0.0\r\n-        \r\n-        for epoch in range(epochs):\r\n-            self.network.set_train_mode(True)\r\n-            \r\n-            idx = np.random.permutation(train_size)\r\n-            x_shuffled = x_train[idx]\r\n-            y_shuffled = y_train[idx]\r\n-            \r\n-            epoch_loss = 0\r\n-            for i in range(iter_per_epoch):\r\n-                start = i * batch_size\r\n-                end = start + batch_size\r\n-                x_batch = x_shuffled[start:end]\r\n-                y_batch = y_shuffled[start:end]\r\n-                \r\n-                loss = self.train_step(x_batch, y_batch)\r\n-                epoch_loss += loss\r\n-            \r\n-            avg_loss = epoch_loss / iter_per_epoch\r\n-            self.train_loss_list.append(avg_loss)\r\n-            \r\n-            self.network.set_train_mode(False)\r\n-            train_acc = self.network.accuracy(x_train, y_train)\r\n-            self.train_acc_list.append(train_acc)\r\n-            \r\n-            if x_test is not None and y_test is not None:\r\n-                test_acc = self.network.accuracy(x_test, y_test)\r\n-                self.test_acc_list.append(test_acc)\r\n-                \r\n-                if early_stopping:\r\n-                    if test_acc > self.best_test_acc:\r\n-                        self.best_test_acc = test_acc\r\n-                        patience_counter = 0\r\n-                        # Save best params\r\n-                        self.best_params = self._save_params()\r\n-                    else:\r\n-                        patience_counter += 1\r\n-                    \r\n-                    if patience_counter >= patience:\r\n-                        if verbose:\r\n-                            print(f\"\\n Early stopping at epoch {epoch + 1}\")\r\n-                            print(f\"Best test accuracy: {self.best_test_acc:.4f}\")\r\n-                        # Restore best params\r\n-                        self._restore_params(self.best_params)\r\n-                        break\r\n-                \r\n-                if verbose:\r\n-                    gap = train_acc - test_acc\r\n-                    print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n-                          f\"Loss: {avg_loss:.4f} - \"\r\n-                          f\"Train Acc: {train_acc:.4f} - \"\r\n-                          f\"Test Acc: {test_acc:.4f} - \"\r\n-                          f\"Gap: {gap:.4f}\")\r\n-            else:\r\n-                if verbose:\r\n-                    print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n-                          f\"Loss: {avg_loss:.4f} - \"\r\n-                          f\"Train Acc: {train_acc:.4f}\")\r\n-            \r\n-            if lr_decay and (epoch + 1) % decay_every == 0:\r\n-                if hasattr(self.optimizer, 'lr'):\r\n-                    self.optimizer.lr *= decay_rate\r\n-                    if verbose:\r\n-                        print(f\"üìâ Learning rate decayed to {self.optimizer.lr:.6f}\")\r\n-    \r\n-    def _save_params(self):\r\n-        \"\"\"ÿ≠ŸÅÿ∏ parameters ÿßŸÑÿ¥ÿ®ŸÉÿ©\"\"\"\r\n-        saved = []\r\n-        for layer in self.network.layers:\r\n-            if hasattr(layer, 'params') and layer.params:\r\n-                layer_params = {}\r\n-                for key, val in layer.params.items():\r\n-                    layer_params[key] = val.copy()\r\n-                saved.append(layer_params)\r\n-            else:\r\n-                saved.append(None)\r\n-        return saved\r\n-    \r\n     def _restore_params(self, saved_params):\r\n-        \"\"\"ÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ parameters ÿßŸÑŸÖÿ≠ŸÅŸàÿ∏ÿ©\"\"\"\r\n         for layer, params in zip(self.network.layers, saved_params):\r\n             if params is not None:\r\n                 for key, val in params.items():\r\n                     layer.params[key] = val.copy()\r\n-                    # Update layer attributes\r\n                     if hasattr(layer, key):\r\n                         setattr(layer, key, val.copy())\r\n"
                }
            ],
            "date": 1768337537716,
            "name": "Commit-0",
            "content": "import numpy as np\r\n\r\n\r\nclass Trainer:\r\n    \"\"\"\r\n    Trainer class ŸÖÿ≠ÿ≥ŸëŸÜ ŸÖÿπ:\r\n    - Early Stopping ŸÑŸÖŸÜÿπ overfitting\r\n    - L2 Regularization\r\n    - Learning Rate Scheduling\r\n    - Model Checkpointing\r\n    \"\"\"\r\n    \r\n    def __init__(self, network, optimizer, weight_decay=0.0):\r\n        \"\"\"\r\n        Args:\r\n            network: NeuralNetwork object\r\n            optimizer: Optimizer object (SGD, Adam, etc.)\r\n            weight_decay: L2 regularization coefficient (default: 0.0)\r\n        \"\"\"\r\n        self.network = network\r\n        self.optimizer = optimizer\r\n        self.weight_decay = weight_decay\r\n        \r\n        # Training history\r\n        self.train_loss_list = []\r\n        self.train_acc_list = []\r\n        self.test_acc_list = []\r\n        \r\n        # Early stopping\r\n        self.best_test_acc = 0.0\r\n        self.best_params = None\r\n        \r\n    def train_step(self, x_batch, y_batch):\r\n        \"\"\"\r\n        ÿÆÿ∑Ÿàÿ© ÿ™ÿØÿ±Ÿäÿ® Ÿàÿßÿ≠ÿØÿ© ŸÖÿπ L2 regularization\r\n        \"\"\"\r\n        # Forward + backward\r\n        loss = self.network.forward(x_batch, y_batch)\r\n        self.network.backward()\r\n        \r\n        # Get params and grads\r\n        params_list, grads_list = self.network.get_params_and_grads()\r\n        \r\n        # Apply L2 regularization to gradients\r\n        if self.weight_decay > 0:\r\n            for params, grads in zip(params_list, grads_list):\r\n                if 'W' in grads:  # Apply only to weights, not biases\r\n                    grads['W'] += self.weight_decay * params['W']\r\n        \r\n        # Update parameters\r\n        for params, grads in zip(params_list, grads_list):\r\n            self.optimizer.update(params, grads)\r\n        \r\n        # Add L2 penalty to loss\r\n        if self.weight_decay > 0:\r\n            l2_loss = 0\r\n            for params in params_list:\r\n                if 'W' in params:\r\n                    l2_loss += np.sum(params['W'] ** 2)\r\n            loss += 0.5 * self.weight_decay * l2_loss\r\n        \r\n        return loss\r\n    \r\n    def fit(self, x_train, y_train, x_test=None, y_test=None, \r\n            epochs=10, batch_size=32, verbose=True, \r\n            early_stopping=False, patience=10, \r\n            lr_decay=False, decay_rate=0.95, decay_every=10):\r\n        \"\"\"\r\n        ÿ™ÿØÿ±Ÿäÿ® ÿßŸÑÿ¥ÿ®ŸÉÿ© ŸÖÿπ ŸÖŸäÿ≤ÿßÿ™ ŸÖÿ™ŸÇÿØŸÖÿ©\r\n        \r\n        Args:\r\n            x_train, y_train: Training data\r\n            x_test, y_test: Test data (optional)\r\n            epochs: ÿπÿØÿØ ÿßŸÑŸÄ epochs\r\n            batch_size: ÿ≠ÿ¨ŸÖ ÿßŸÑŸÄ batch\r\n            verbose: ÿ∑ÿ®ÿßÿπÿ© ÿßŸÑÿ™ŸÇÿØŸÖ\r\n            early_stopping: ÿ™ŸÅÿπŸäŸÑ early stopping\r\n            patience: ÿπÿØÿØ epochs ŸÑŸÑÿßŸÜÿ™ÿ∏ÿßÿ± ŸÇÿ®ŸÑ ÿßŸÑÿ™ŸàŸÇŸÅ\r\n            lr_decay: ÿ™ŸÅÿπŸäŸÑ learning rate decay\r\n            decay_rate: ŸÖÿπÿØŸÑ ÿßŸÑÿßŸÜÿÆŸÅÿßÿ∂\r\n            decay_every: ŸÉŸÑ ŸÉŸÖ epoch ŸäŸÜÿÆŸÅÿ∂ ÿßŸÑŸÄ LR\r\n        \"\"\"\r\n        train_size = x_train.shape[0]\r\n        iter_per_epoch = max(train_size // batch_size, 1)\r\n        \r\n        # Early stopping\r\n        patience_counter = 0\r\n        self.best_test_acc = 0.0\r\n        \r\n        for epoch in range(epochs):\r\n            # Set training mode\r\n            self.network.set_train_mode(True)\r\n            \r\n            # Shuffle data\r\n            idx = np.random.permutation(train_size)\r\n            x_shuffled = x_train[idx]\r\n            y_shuffled = y_train[idx]\r\n            \r\n            # Training loop\r\n            epoch_loss = 0\r\n            for i in range(iter_per_epoch):\r\n                start = i * batch_size\r\n                end = start + batch_size\r\n                x_batch = x_shuffled[start:end]\r\n                y_batch = y_shuffled[start:end]\r\n                \r\n                loss = self.train_step(x_batch, y_batch)\r\n                epoch_loss += loss\r\n            \r\n            avg_loss = epoch_loss / iter_per_epoch\r\n            self.train_loss_list.append(avg_loss)\r\n            \r\n            # Evaluation\r\n            self.network.set_train_mode(False)\r\n            train_acc = self.network.accuracy(x_train, y_train)\r\n            self.train_acc_list.append(train_acc)\r\n            \r\n            # Test accuracy\r\n            if x_test is not None and y_test is not None:\r\n                test_acc = self.network.accuracy(x_test, y_test)\r\n                self.test_acc_list.append(test_acc)\r\n                \r\n                # Early stopping check\r\n                if early_stopping:\r\n                    if test_acc > self.best_test_acc:\r\n                        self.best_test_acc = test_acc\r\n                        patience_counter = 0\r\n                        # Save best params\r\n                        self.best_params = self._save_params()\r\n                    else:\r\n                        patience_counter += 1\r\n                    \r\n                    if patience_counter >= patience:\r\n                        if verbose:\r\n                            print(f\"\\n‚ö†Ô∏è Early stopping at epoch {epoch + 1}\")\r\n                            print(f\"Best test accuracy: {self.best_test_acc:.4f}\")\r\n                        # Restore best params\r\n                        self._restore_params(self.best_params)\r\n                        break\r\n                \r\n                if verbose:\r\n                    gap = train_acc - test_acc\r\n                    gap_indicator = \"üî¥\" if gap > 0.05 else \"‚úÖ\"\r\n                    print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n                          f\"Loss: {avg_loss:.4f} - \"\r\n                          f\"Train Acc: {train_acc:.4f} - \"\r\n                          f\"Test Acc: {test_acc:.4f} - \"\r\n                          f\"Gap: {gap:.4f} {gap_indicator}\")\r\n            else:\r\n                if verbose:\r\n                    print(f\"Epoch {epoch + 1}/{epochs} - \"\r\n                          f\"Loss: {avg_loss:.4f} - \"\r\n                          f\"Train Acc: {train_acc:.4f}\")\r\n            \r\n            # Learning rate decay\r\n            if lr_decay and (epoch + 1) % decay_every == 0:\r\n                if hasattr(self.optimizer, 'lr'):\r\n                    self.optimizer.lr *= decay_rate\r\n                    if verbose:\r\n                        print(f\"üìâ Learning rate decayed to {self.optimizer.lr:.6f}\")\r\n    \r\n    def _save_params(self):\r\n        \"\"\"ÿ≠ŸÅÿ∏ parameters ÿßŸÑÿ¥ÿ®ŸÉÿ©\"\"\"\r\n        saved = []\r\n        for layer in self.network.layers:\r\n            if hasattr(layer, 'params') and layer.params:\r\n                layer_params = {}\r\n                for key, val in layer.params.items():\r\n                    layer_params[key] = val.copy()\r\n                saved.append(layer_params)\r\n            else:\r\n                saved.append(None)\r\n        return saved\r\n    \r\n    def _restore_params(self, saved_params):\r\n        \"\"\"ÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ parameters ÿßŸÑŸÖÿ≠ŸÅŸàÿ∏ÿ©\"\"\"\r\n        for layer, params in zip(self.network.layers, saved_params):\r\n            if params is not None:\r\n                for key, val in params.items():\r\n                    layer.params[key] = val.copy()\r\n                    # Update layer attributes\r\n                    if hasattr(layer, key):\r\n                        setattr(layer, key, val.copy())\r\n"
        }
    ]
}