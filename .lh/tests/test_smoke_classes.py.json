{
    "sourceFile": "tests/test_smoke_classes.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1768338791376,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1768338791376,
            "name": "Commit-0",
            "content": "import numpy as np\r\nfrom core.network import NeuralNetwork\r\nfrom core.base_layer import Layer\r\n\r\nfrom layers.dense import DenseLayer\r\nfrom layers.activations import ReLU, Sigmoid, Tanh, LinearActivation\r\nfrom layers.normalization import Dropout, BatchNorm\r\n\r\nfrom losses.loss_functions import MeanSquaredError, SoftmaxCrossEntropy\r\n\r\nfrom optimizers.optimizer_classes import SGD, Momentum, AdaGrad, Adam\r\n\r\nfrom training.trainer import Trainer\r\nfrom training.tuner import HyperparameterTuner\r\n\r\n\r\ndef test_dense_and_activation_forward_backward():\r\n    x = np.random.randn(6, 4)\r\n    d = DenseLayer(4, 3)\r\n    out = d.forward(x)\r\n    assert out.shape == (6, 3)\r\n\r\n    dout = np.random.randn(6, 3)\r\n    dx = d.backward(dout)\r\n    assert dx.shape == x.shape\r\n\r\n    relu = ReLU()\r\n    r = relu.forward(out)\r\n    assert r.shape == out.shape\r\n    assert relu.backward(dout).shape == dout.shape\r\n\r\n    sig = Sigmoid()\r\n    s = sig.forward(out)\r\n    assert s.shape == out.shape\r\n    assert sig.backward(dout).shape == dout.shape\r\n\r\n    tan = Tanh()\r\n    t = tan.forward(out)\r\n    assert t.shape == out.shape\r\n    assert tan.backward(dout).shape == dout.shape\r\n\r\n\r\ndef test_dropout_and_batchnorm():\r\n    x = np.random.randn(5, 4)\r\n    drop = Dropout(0.5)\r\n    drop.train_mode = True\r\n    y = drop.forward(x)\r\n    assert y.shape == x.shape\r\n    drop.backward(np.ones_like(x))\r\n\r\n    bn = BatchNorm(4)\r\n    bn.train_mode = True\r\n    y2 = bn.forward(x)\r\n    assert y2.shape == x.shape\r\n    dx = bn.backward(np.ones_like(x))\r\n    assert dx.shape == x.shape\r\n\r\n\r\ndef test_losses():\r\n    y_true = np.array([0, 1, 2])\r\n    x = np.random.randn(3, 3)\r\n    sce = SoftmaxCrossEntropy()\r\n    loss = sce.forward(x, y_true)\r\n    assert isinstance(loss, float)\r\n    dx = sce.backward()\r\n    assert dx.shape == x.shape\r\n\r\n    y_pred = np.random.randn(4, 2)\r\n    y_t = np.random.randn(4, 2)\r\n    mse = MeanSquaredError()\r\n    loss2 = mse.forward(y_pred, y_t)\r\n    assert isinstance(loss2, float)\r\n    dx2 = mse.backward()\r\n    assert dx2.shape == y_pred.shape\r\n\r\n\r\ndef test_optimizers_update():\r\n    params = {'W': np.ones((2, 2)), 'b': np.zeros((1, 2))}\r\n    grads = {'W': np.full((2, 2), 0.1), 'b': np.full((1, 2), 0.1)}\r\n\r\n    for Optim in (SGD, Momentum, AdaGrad, Adam):\r\n        opt = Optim()\r\n        p_before = {k: v.copy() for k, v in params.items()}\r\n        opt.update(params, grads)\r\n        for k in params:\r\n            assert params[k].shape == p_before[k].shape\r\n\r\n\r\ndef test_network_trainer_basic_flow():\r\n    nn = NeuralNetwork()\r\n    nn.add_layer(DenseLayer(4, 5))\r\n    nn.add_layer(ReLU())\r\n    nn.add_layer(DenseLayer(5, 3))\r\n    nn.set_loss(SoftmaxCrossEntropy())\r\n\r\n    x = np.random.randn(10, 4)\r\n    y = np.random.randint(0, 3, size=(10,))\r\n\r\n    loss = nn.forward(x, y)\r\n    assert isinstance(loss, float)\r\n    nn.backward()\r\n\r\n    params, grads = nn.get_params_and_grads()\r\n    assert isinstance(params, list)\r\n    assert isinstance(grads, list)\r\n\r\n    trainer = Trainer(nn, SGD(lr=0.01))\r\n    loss2 = trainer.train_step(x[:4], y[:4])\r\n    assert isinstance(loss2, float)\r\n\r\n\r\ndef test_hyperparameter_tuner_smoke():\r\n    def build_model(params):\r\n        nn = NeuralNetwork()\r\n        nn.add_layer(DenseLayer(4, params.get('hidden', 5)))\r\n        nn.add_layer(ReLU())\r\n        nn.add_layer(DenseLayer(params.get('hidden', 5), 3))\r\n        opt = SGD(lr=params.get('lr', 0.01))\r\n        nn.set_loss(SoftmaxCrossEntropy())\r\n        return nn, opt\r\n\r\n    x = np.random.randn(20, 4)\r\n    y = np.random.randint(0, 3, size=(20,))\r\n\r\n    tuner = HyperparameterTuner()\r\n    best_params, best_score = tuner.grid_search(\r\n        build_model, x, y, x, y, {'hidden': [4], 'lr': [0.01]}, epochs=1, batch_size=8)\r\n    assert isinstance(best_params, dict)\r\n    assert isinstance(best_score, float)\r\n"
        }
    ]
}