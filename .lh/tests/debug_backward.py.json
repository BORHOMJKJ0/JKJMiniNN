{
    "sourceFile": "tests/debug_backward.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1768336965149,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1768336965149,
            "name": "Commit-0",
            "content": "import numpy as np\r\n\r\nfrom core.network import NeuralNetwork\r\nfrom layers.dense import DenseLayer\r\nfrom layers.activations import ReLU, Sigmoid\r\nfrom layers.normalization import BatchNorm\r\nfrom losses.loss_functions import SoftmaxCrossEntropy\r\n\r\nnet = NeuralNetwork()\r\nnet.add_layer(DenseLayer(4, 16))\r\nnet.add_layer(Sigmoid())\r\nnet.add_layer(BatchNorm(16))\r\nnet.add_layer(DenseLayer(16, 8))\r\nnet.add_layer(ReLU())\r\nnet.add_layer(DenseLayer(8, 3))\r\nnet.set_loss(SoftmaxCrossEntropy())\r\n\r\nX = np.random.randn(5, 4)\r\ny = np.array([0, 1, 2, 1, 0])\r\n\r\nloss = net.forward(X, y)\r\nprint('Loss:', loss)\r\nnet.backward()\r\n\r\nfor i, layer in enumerate(net.layers):\r\n    print('Layer', i, type(layer).__name__)\r\n    print(' params keys:', list(getattr(layer, 'params', {}).keys()))\r\n    print(' grads keys:', list(getattr(layer, 'grads', {}).keys()))\r\n    print('')\r\n"
        }
    ]
}