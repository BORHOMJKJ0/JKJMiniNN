{
    "sourceFile": "layers/normalization.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 2,
            "patches": [
                {
                    "date": 1768335587831,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1768335604520,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,71 @@\n+import numpy as np\r\n+from core.base_layer import Layer\r\n+\r\n+\r\n+class Dropout(Layer):\r\n+    def __init__(self, dropout_rate=0.5):\r\n+        super().__init__()\r\n+        self.dropout_rate = dropout_rate\r\n+        self.mask = None\r\n+        self.train_mode = True\r\n+\r\n+    def forward(self, x):\r\n+        if self.train_mode:\r\n+            self.mask = np.random.rand(*x.shape) > self.dropout_rate\r\n+            return x * self.mask\r\n+        else:\r\n+            return x * (1.0 - self.dropout_rate)\r\n+\r\n+    def backward(self, dout):\r\n+        return dout * self.mask\r\n+\r\n+\r\n+class BatchNorm(Layer):\r\n+    def __init__(self, input_dim, momentum=0.9, eps=1e-5):\r\n+        super().__init__()\r\n+        self.gamma = np.ones((1, input_dim))\r\n+        self.beta = np.zeros((1, input_dim))\r\n+        self.params = {'gamma': self.gamma, 'beta': self.beta}\r\n+\r\n+        self.momentum = momentum\r\n+        self.eps = eps\r\n+        self.running_mean = np.zeros((1, input_dim))\r\n+        self.running_var = np.ones((1, input_dim))\r\n+\r\n+        self.train_mode = True\r\n+        self.xc = None\r\n+        self.std = None\r\n+        self.xn = None\r\n+\r\n+    def forward(self, x):\r\n+        if self.train_mode:\r\n+            mu = np.mean(x, axis=0, keepdims=True)\r\n+            var = np.var(x, axis=0, keepdims=True)\r\n+\r\n+            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mu\r\n+            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\r\n+\r\n+            self.xc = x - mu\r\n+            self.std = np.sqrt(var + self.eps)\r\n+            self.xn = self.xc / self.std\r\n+\r\n+            out = self.gamma * self.xn + self.beta\r\n+        else:\r\n+            xc = x - self.running_mean\r\n+            xn = xc / np.sqrt(self.running_var + self.eps)\r\n+            out = self.gamma * xn + self.beta\r\n+\r\n+        return out\r\n+\r\n+    def backward(self, dout):\r\n+        N = dout.shape[0]\r\n+\r\n+        dbeta = np.sum(dout, axis=0, keepdims=True)\r\n+        dgamma = np.sum(dout * self.xn, axis=0, keepdims=True)\r\n+\r\n+        dx = (1.0 / N) * (self.gamma / self.std) * (\r\n+            N * dout - np.sum(dout, axis=0, keepdims=True) - self.xn * np.sum(dout * self.xn, axis=0, keepdims=True)\r\n+        )\r\n+\r\n+        self.grads = {'gamma': dgamma, 'beta': dbeta}\r\n+        return dx\n\\ No newline at end of file\n"
                },
                {
                    "date": 1768338741707,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -41,10 +41,12 @@\n         if self.train_mode:\r\n             mu = np.mean(x, axis=0, keepdims=True)\r\n             var = np.var(x, axis=0, keepdims=True)\r\n \r\n-            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mu\r\n-            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\r\n+            self.running_mean = self.momentum * \\\r\n+                self.running_mean + (1 - self.momentum) * mu\r\n+            self.running_var = self.momentum * \\\r\n+                self.running_var + (1 - self.momentum) * var\r\n \r\n             self.xc = x - mu\r\n             self.std = np.sqrt(var + self.eps)\r\n             self.xn = self.xc / self.std\r\n@@ -63,9 +65,10 @@\n         dbeta = np.sum(dout, axis=0, keepdims=True)\r\n         dgamma = np.sum(dout * self.xn, axis=0, keepdims=True)\r\n \r\n         dx = (1.0 / N) * (self.gamma / self.std) * (\r\n-            N * dout - np.sum(dout, axis=0, keepdims=True) - self.xn * np.sum(dout * self.xn, axis=0, keepdims=True)\r\n+            N * dout - np.sum(dout, axis=0, keepdims=True) -\r\n+            self.xn * np.sum(dout * self.xn, axis=0, keepdims=True)\r\n         )\r\n \r\n\\ No newline at end of file\n         self.grads = {'gamma': dgamma, 'beta': dbeta}\r\n-        return dx\n+        return dx\r\n"
                }
            ],
            "date": 1768335587831,
            "name": "Commit-0",
            "content": ""
        }
    ]
}