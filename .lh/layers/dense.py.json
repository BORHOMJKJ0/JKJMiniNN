{
    "sourceFile": "layers/dense.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 8,
            "patches": [
                {
                    "date": 1768337567767,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1768338388400,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,42 +2,23 @@\n from core.base_layer import Layer\r\n \r\n \r\n class DenseLayer(Layer):\r\n-    \"\"\"\r\n-    Dense (Fully Connected) Layer مع دعم weight initialization\r\n-    \"\"\"\r\n-    \r\n-    def __init__(self, n_in, n_out, weight_init='he'):\r\n-        \"\"\"\r\n-        Args:\r\n-            n_in: عدد الـ inputs\r\n-            n_out: عدد الـ outputs\r\n-            weight_init: طريقة التهيئة ('he', 'xavier', 'normal')\r\n-        \"\"\"\r\n+    def __init__(self, n_in, n_out):\r\n         super().__init__()\r\n-        \r\n-        # Weight initialization\r\n-        if weight_init == 'he':  # للـ ReLU\r\n-            self.W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)\r\n-        elif weight_init == 'xavier':  # للـ Sigmoid/Tanh\r\n-            self.W = np.random.randn(n_in, n_out) * np.sqrt(1.0 / n_in)\r\n-        else:  # normal\r\n-            self.W = np.random.randn(n_in, n_out) * 0.01\r\n-        \r\n+        self.W = np.random.randn(n_in, n_out) * 0.01\r\n         self.b = np.zeros((1, n_out))\r\n         self.params = {'W': self.W, 'b': self.b}\r\n         self.x_input = None\r\n-    \r\n+\r\n     def forward(self, x):\r\n         self.x_input = x\r\n         out = np.dot(x, self.W) + self.b\r\n         return out\r\n-    \r\n+\r\n     def backward(self, dout):\r\n         dx = np.dot(dout, self.W.T)\r\n         dW = np.dot(self.x_input.T, dout)\r\n         db = np.sum(dout, axis=0, keepdims=True)\r\n-        \r\n+\r\n         self.grads = {'W': dW, 'b': db}\r\n-        return dx\r\n-\r\n+        return dx\n\\ No newline at end of file\n"
                },
                {
                    "date": 1768338418511,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,37 @@\n+import numpy as np\r\n+from core.base_layer import Layer\r\n+\r\n+\r\n+class DenseLayer(Layer):\r\n+    \"\"\"\r\n+    Dense (Fully Connected) Layer مع دعم weight initialization\r\n+    \"\"\"\r\n+    \r\n+    def __init__(self, n_in, n_out, weight_init='he'):\r\n+        super().__init__()\r\n+        \r\n+        # Weight initialization\r\n+        if weight_init == 'he':  # للـ ReLU\r\n+            self.W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)\r\n+        elif weight_init == 'xavier':  # للـ Sigmoid/Tanh\r\n+            self.W = np.random.randn(n_in, n_out) * np.sqrt(1.0 / n_in)\r\n+        else:  # normal\r\n+            self.W = np.random.randn(n_in, n_out) * 0.01\r\n+        \r\n+        self.b = np.zeros((1, n_out))\r\n+        self.params = {'W': self.W, 'b': self.b}\r\n+        self.x_input = None\r\n+    \r\n+    def forward(self, x):\r\n+        self.x_input = x\r\n+        out = np.dot(x, self.W) + self.b\r\n+        return out\r\n+    \r\n+    def backward(self, dout):\r\n+        dx = np.dot(dout, self.W.T)\r\n+        dW = np.dot(self.x_input.T, dout)\r\n+        db = np.sum(dout, axis=0, keepdims=True)\r\n+        \r\n+        self.grads = {'W': dW, 'b': db}\r\n+        return dx\r\n+\r\n"
                },
                {
                    "date": 1768338425091,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,12 +2,8 @@\n from core.base_layer import Layer\r\n \r\n \r\n class DenseLayer(Layer):\r\n-    \"\"\"\r\n-    Dense (Fully Connected) Layer مع دعم weight initialization\r\n-    \"\"\"\r\n-    \r\n     def __init__(self, n_in, n_out, weight_init='he'):\r\n         super().__init__()\r\n         \r\n         # Weight initialization\r\n@@ -34,28 +30,4 @@\n         \r\n         self.grads = {'W': dW, 'b': db}\r\n         return dx\r\n \r\n-import numpy as np\r\n-from core.base_layer import Layer\r\n-\r\n-\r\n-class DenseLayer(Layer):\r\n-    def __init__(self, n_in, n_out):\r\n-        super().__init__()\r\n-        self.W = np.random.randn(n_in, n_out) * 0.01\r\n-        self.b = np.zeros((1, n_out))\r\n-        self.params = {'W': self.W, 'b': self.b}\r\n-        self.x_input = None\r\n-\r\n-    def forward(self, x):\r\n-        self.x_input = x\r\n-        out = np.dot(x, self.W) + self.b\r\n-        return out\r\n-\r\n-    def backward(self, dout):\r\n-        dx = np.dot(dout, self.W.T)\r\n-        dW = np.dot(self.x_input.T, dout)\r\n-        db = np.sum(dout, axis=0, keepdims=True)\r\n-\r\n-        self.grads = {'W': dW, 'b': db}\r\n-        return dx\n\\ No newline at end of file\n"
                },
                {
                    "date": 1768338430489,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,11 +6,11 @@\n     def __init__(self, n_in, n_out, weight_init='he'):\r\n         super().__init__()\r\n         \r\n         # Weight initialization\r\n-        if weight_init == 'he':  # للـ ReLU\r\n+        if weight_init == 'he':  \r\n             self.W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)\r\n-        elif weight_init == 'xavier':  # للـ Sigmoid/Tanh\r\n+        elif weight_init == 'xavier':  \r\n             self.W = np.random.randn(n_in, n_out) * np.sqrt(1.0 / n_in)\r\n         else:  # normal\r\n             self.W = np.random.randn(n_in, n_out) * 0.01\r\n         \r\n"
                },
                {
                    "date": 1768338437542,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,32 @@\n+import numpy as np\r\n+from core.base_layer import Layer\r\n+\r\n+\r\n+class DenseLayer(Layer):\r\n+    def __init__(self, n_in, n_out, weight_init='he'):\r\n+        super().__init__()\r\n+        \r\n+        if weight_init == 'he':  \r\n+            self.W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)\r\n+        elif weight_init == 'xavier':  \r\n+            self.W = np.random.randn(n_in, n_out) * np.sqrt(1.0 / n_in)\r\n+        else:  \r\n+            self.W = np.random.randn(n_in, n_out) * 0.01\r\n+        \r\n+        self.b = np.zeros((1, n_out))\r\n+        self.params = {'W': self.W, 'b': self.b}\r\n+        self.x_input = None\r\n+    \r\n+    def forward(self, x):\r\n+        self.x_input = x\r\n+        out = np.dot(x, self.W) + self.b\r\n+        return out\r\n+    \r\n+    def backward(self, dout):\r\n+        dx = np.dot(dout, self.W.T)\r\n+        dW = np.dot(self.x_input.T, dout)\r\n+        db = np.sum(dout, axis=0, keepdims=True)\r\n+        \r\n+        self.grads = {'W': dW, 'b': db}\r\n+        return dx\r\n+\r\n"
                },
                {
                    "date": 1768338443729,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,32 @@\n+import numpy as np\r\n+from core.base_layer import Layer\r\n+\r\n+\r\n+class DenseLayer(Layer):\r\n+    def __init__(self, n_in, n_out, weight_init='he'):\r\n+        super().__init__()\r\n+        \r\n+        if weight_init == 'he':  \r\n+            self.W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)\r\n+        elif weight_init == 'xavier':  \r\n+            self.W = np.random.randn(n_in, n_out) * np.sqrt(1.0 / n_in)\r\n+        else:  \r\n+            self.W = np.random.randn(n_in, n_out) * 0.01\r\n+        \r\n+        self.b = np.zeros((1, n_out))\r\n+        self.params = {'W': self.W, 'b': self.b}\r\n+        self.x_input = None\r\n+    \r\n+    def forward(self, x):\r\n+        self.x_input = x\r\n+        out = np.dot(x, self.W) + self.b\r\n+        return out\r\n+    \r\n+    def backward(self, dout):\r\n+        dx = np.dot(dout, self.W.T)\r\n+        dW = np.dot(self.x_input.T, dout)\r\n+        db = np.sum(dout, axis=0, keepdims=True)\r\n+        \r\n+        self.grads = {'W': dW, 'b': db}\r\n+        return dx\r\n+\r\n"
                },
                {
                    "date": 1768338729494,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,94 +4,29 @@\n \r\n class DenseLayer(Layer):\r\n     def __init__(self, n_in, n_out, weight_init='he'):\r\n         super().__init__()\r\n-        \r\n-        if weight_init == 'he':  \r\n+\r\n+        if weight_init == 'he':\r\n             self.W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)\r\n-        elif weight_init == 'xavier':  \r\n+        elif weight_init == 'xavier':\r\n             self.W = np.random.randn(n_in, n_out) * np.sqrt(1.0 / n_in)\r\n-        else:  \r\n+        else:\r\n             self.W = np.random.randn(n_in, n_out) * 0.01\r\n-        \r\n+\r\n         self.b = np.zeros((1, n_out))\r\n         self.params = {'W': self.W, 'b': self.b}\r\n         self.x_input = None\r\n-    \r\n+\r\n     def forward(self, x):\r\n         self.x_input = x\r\n         out = np.dot(x, self.W) + self.b\r\n         return out\r\n-    \r\n-    def backward(self, dout):\r\n-        dx = np.dot(dout, self.W.T)\r\n-        dW = np.dot(self.x_input.T, dout)\r\n-        db = np.sum(dout, axis=0, keepdims=True)\r\n-        \r\n-        self.grads = {'W': dW, 'b': db}\r\n-        return dx\r\n \r\n-import numpy as np\r\n-from core.base_layer import Layer\r\n-\r\n-\r\n-class DenseLayer(Layer):\r\n-    def __init__(self, n_in, n_out, weight_init='he'):\r\n-        super().__init__()\r\n-        \r\n-        if weight_init == 'he':  \r\n-            self.W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)\r\n-        elif weight_init == 'xavier':  \r\n-            self.W = np.random.randn(n_in, n_out) * np.sqrt(1.0 / n_in)\r\n-        else:  \r\n-            self.W = np.random.randn(n_in, n_out) * 0.01\r\n-        \r\n-        self.b = np.zeros((1, n_out))\r\n-        self.params = {'W': self.W, 'b': self.b}\r\n-        self.x_input = None\r\n-    \r\n-    def forward(self, x):\r\n-        self.x_input = x\r\n-        out = np.dot(x, self.W) + self.b\r\n-        return out\r\n-    \r\n     def backward(self, dout):\r\n         dx = np.dot(dout, self.W.T)\r\n         dW = np.dot(self.x_input.T, dout)\r\n         db = np.sum(dout, axis=0, keepdims=True)\r\n-        \r\n-        self.grads = {'W': dW, 'b': db}\r\n-        return dx\r\n \r\n-import numpy as np\r\n-from core.base_layer import Layer\r\n-\r\n-\r\n-class DenseLayer(Layer):\r\n-    def __init__(self, n_in, n_out, weight_init='he'):\r\n-        super().__init__()\r\n-        \r\n-        # Weight initialization\r\n-        if weight_init == 'he':  \r\n-            self.W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)\r\n-        elif weight_init == 'xavier':  \r\n-            self.W = np.random.randn(n_in, n_out) * np.sqrt(1.0 / n_in)\r\n-        else:  # normal\r\n-            self.W = np.random.randn(n_in, n_out) * 0.01\r\n-        \r\n-        self.b = np.zeros((1, n_out))\r\n-        self.params = {'W': self.W, 'b': self.b}\r\n-        self.x_input = None\r\n-    \r\n-    def forward(self, x):\r\n-        self.x_input = x\r\n-        out = np.dot(x, self.W) + self.b\r\n-        return out\r\n-    \r\n-    def backward(self, dout):\r\n-        dx = np.dot(dout, self.W.T)\r\n-        dW = np.dot(self.x_input.T, dout)\r\n-        db = np.sum(dout, axis=0, keepdims=True)\r\n-        \r\n         self.grads = {'W': dW, 'b': db}\r\n         return dx\r\n \r\n"
                },
                {
                    "date": 1768338734872,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,5 +28,4 @@\n         db = np.sum(dout, axis=0, keepdims=True)\r\n \r\n         self.grads = {'W': dW, 'b': db}\r\n         return dx\r\n-\r\n"
                }
            ],
            "date": 1768337567767,
            "name": "Commit-0",
            "content": "import numpy as np\r\nfrom core.base_layer import Layer\r\n\r\n\r\nclass DenseLayer(Layer):\r\n    \"\"\"\r\n    Dense (Fully Connected) Layer مع دعم weight initialization\r\n    \"\"\"\r\n    \r\n    def __init__(self, n_in, n_out, weight_init='he'):\r\n        \"\"\"\r\n        Args:\r\n            n_in: عدد الـ inputs\r\n            n_out: عدد الـ outputs\r\n            weight_init: طريقة التهيئة ('he', 'xavier', 'normal')\r\n        \"\"\"\r\n        super().__init__()\r\n        \r\n        # Weight initialization\r\n        if weight_init == 'he':  # للـ ReLU\r\n            self.W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)\r\n        elif weight_init == 'xavier':  # للـ Sigmoid/Tanh\r\n            self.W = np.random.randn(n_in, n_out) * np.sqrt(1.0 / n_in)\r\n        else:  # normal\r\n            self.W = np.random.randn(n_in, n_out) * 0.01\r\n        \r\n        self.b = np.zeros((1, n_out))\r\n        self.params = {'W': self.W, 'b': self.b}\r\n        self.x_input = None\r\n    \r\n    def forward(self, x):\r\n        self.x_input = x\r\n        out = np.dot(x, self.W) + self.b\r\n        return out\r\n    \r\n    def backward(self, dout):\r\n        dx = np.dot(dout, self.W.T)\r\n        dW = np.dot(self.x_input.T, dout)\r\n        db = np.sum(dout, axis=0, keepdims=True)\r\n        \r\n        self.grads = {'W': dW, 'b': db}\r\n        return dx\r\n\r\n"
        }
    ]
}