{
    "sourceFile": "examples/advanced_network.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 4,
            "patches": [
                {
                    "date": 1768335944945,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1768335954484,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,9 +20,9 @@\n \r\n print(\"Building advanced network with regularization...\")\r\n network = NeuralNetwork()\r\n network.add_layer(DenseLayer(64, 256))\r\n-network.add_layer(BatchNorm(256))    # BatchNorm بعد Dense مباشرة\r\n+network.add_layer(BatchNorm(256))    #\r\n network.add_layer(ReLU())            # Activation بعد BatchNorm\r\n network.add_layer(Dropout(0.3))\r\n \r\n network.add_layer(DenseLayer(256, 128))\r\n"
                },
                {
                    "date": 1768335964318,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,10 +20,10 @@\n \r\n print(\"Building advanced network with regularization...\")\r\n network = NeuralNetwork()\r\n network.add_layer(DenseLayer(64, 256))\r\n-network.add_layer(BatchNorm(256))    #\r\n-network.add_layer(ReLU())            # Activation بعد BatchNorm\r\n+network.add_layer(BatchNorm(256))\r\n+network.add_layer(ReLU())    \r\n network.add_layer(Dropout(0.3))\r\n \r\n network.add_layer(DenseLayer(256, 128))\r\n network.add_layer(Tanh())\r\n"
                },
                {
                    "date": 1768335995185,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,49 @@\n+import numpy as np\r\n+import sys\r\n+sys.path.append('..')\r\n+\r\n+from utils.data_utils import load_data, split_data, normalize_data\r\n+\r\n+from core.network import NeuralNetwork\r\n+from layers.dense import DenseLayer\r\n+from layers.activations import ReLU, Tanh\r\n+from layers.normalization import Dropout, BatchNorm\r\n+from losses.loss_functions import SoftmaxCrossEntropy\r\n+from optimizers.optimizer_classes import Adam, Momentum\r\n+from training.trainer import Trainer\r\n+\r\n+X, y = load_data('digits')\r\n+\r\n+X, mean, std = normalize_data(X)\r\n+\r\n+X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\r\n+\r\n+print(\"Building advanced network with regularization...\")\r\n+network = NeuralNetwork()\r\n+network.add_layer(DenseLayer(64, 256))\r\n+network.add_layer(BatchNorm(256))\r\n+network.add_layer(ReLU())\r\n+network.add_layer(Dropout(0.3))\r\n+\r\n+network.add_layer(DenseLayer(256, 128))\r\n+network.add_layer(Tanh())\r\n+network.add_layer(BatchNorm(128))\r\n+network.add_layer(Dropout(0.3))\r\n+\r\n+network.add_layer(DenseLayer(128, 64))\r\n+network.add_layer(ReLU())\r\n+network.add_layer(Dropout(0.2))\r\n+\r\n+network.add_layer(DenseLayer(64, 10))\r\n+network.set_loss(SoftmaxCrossEntropy())\r\n+\r\n+optimizer = Adam(lr=0.001, beta1=0.9, beta2=0.999)\r\n+trainer = Trainer(network, optimizer)\r\n+\r\n+print(\"Training advanced network on Digits dataset...\")\r\n+trainer.fit(X_train, y_train, X_test, y_test, epochs=100, batch_size=64, verbose=True)\r\n+\r\n+print(\"\" + \"=\"*60)\r\n+print(f\"Final Training Accuracy: {network.accuracy(X_train, y_train):.4f}\")\r\n+print(f\"Final Test Accuracy: {network.accuracy(X_test, y_test):.4f}\")\r\n+print(\"=\"*60)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1768338670507,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,23 +1,23 @@\n+from training.trainer import Trainer\r\n+from optimizers.optimizer_classes import Adam, Momentum\r\n+from losses.loss_functions import SoftmaxCrossEntropy\r\n+from layers.normalization import Dropout, BatchNorm\r\n+from layers.activations import ReLU, Tanh\r\n+from layers.dense import DenseLayer\r\n+from core.network import NeuralNetwork\r\n+from utils.data_utils import load_data, split_data, normalize_data\r\n import numpy as np\r\n import sys\r\n sys.path.append('..')\r\n \r\n-from utils.data_utils import load_data, split_data, normalize_data\r\n \r\n-from core.network import NeuralNetwork\r\n-from layers.dense import DenseLayer\r\n-from layers.activations import ReLU, Tanh\r\n-from layers.normalization import Dropout, BatchNorm\r\n-from losses.loss_functions import SoftmaxCrossEntropy\r\n-from optimizers.optimizer_classes import Adam, Momentum\r\n-from training.trainer import Trainer\r\n-\r\n X, y = load_data('digits')\r\n \r\n X, mean, std = normalize_data(X)\r\n \r\n-X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\r\n+X_train, X_test, y_train, y_test = split_data(\r\n+    X, y, test_size=0.2, random_state=42)\r\n \r\n print(\"Building advanced network with regularization...\")\r\n network = NeuralNetwork()\r\n network.add_layer(DenseLayer(64, 256))\r\n@@ -40,59 +40,11 @@\n optimizer = Adam(lr=0.001, beta1=0.9, beta2=0.999)\r\n trainer = Trainer(network, optimizer)\r\n \r\n print(\"Training advanced network on Digits dataset...\")\r\n-trainer.fit(X_train, y_train, X_test, y_test, epochs=100, batch_size=64, verbose=True)\r\n+trainer.fit(X_train, y_train, X_test, y_test,\r\n+            epochs=100, batch_size=64, verbose=True)\r\n \r\n print(\"\" + \"=\"*60)\r\n print(f\"Final Training Accuracy: {network.accuracy(X_train, y_train):.4f}\")\r\n print(f\"Final Test Accuracy: {network.accuracy(X_test, y_test):.4f}\")\r\n-print(\"=\"*60)\n-import numpy as np\r\n-import sys\r\n-sys.path.append('..')\r\n-\r\n-from utils.data_utils import load_data, split_data, normalize_data\r\n-\r\n-from core.network import NeuralNetwork\r\n-from layers.dense import DenseLayer\r\n-from layers.activations import ReLU, Tanh\r\n-from layers.normalization import Dropout, BatchNorm\r\n-from losses.loss_functions import SoftmaxCrossEntropy\r\n-from optimizers.optimizer_classes import Adam, Momentum\r\n-from training.trainer import Trainer\r\n-\r\n-X, y = load_data('digits')\r\n-\r\n-X, mean, std = normalize_data(X)\r\n-\r\n-X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\r\n-\r\n-print(\"Building advanced network with regularization...\")\r\n-network = NeuralNetwork()\r\n-network.add_layer(DenseLayer(64, 256))\r\n-network.add_layer(BatchNorm(256))\r\n-network.add_layer(ReLU())    \r\n-network.add_layer(Dropout(0.3))\r\n-\r\n-network.add_layer(DenseLayer(256, 128))\r\n-network.add_layer(Tanh())\r\n-network.add_layer(BatchNorm(128))\r\n-network.add_layer(Dropout(0.3))\r\n-\r\n-network.add_layer(DenseLayer(128, 64))\r\n-network.add_layer(ReLU())\r\n-network.add_layer(Dropout(0.2))\r\n-\r\n-network.add_layer(DenseLayer(64, 10))\r\n-network.set_loss(SoftmaxCrossEntropy())\r\n-\r\n-optimizer = Adam(lr=0.001, beta1=0.9, beta2=0.999)\r\n-trainer = Trainer(network, optimizer)\r\n-\r\n-print(\"Training advanced network on Digits dataset...\")\r\n-trainer.fit(X_train, y_train, X_test, y_test, epochs=100, batch_size=64, verbose=True)\r\n-\r\n-print(\"\" + \"=\"*60)\r\n-print(f\"Final Training Accuracy: {network.accuracy(X_train, y_train):.4f}\")\r\n\\ No newline at end of file\n-print(f\"Final Test Accuracy: {network.accuracy(X_test, y_test):.4f}\")\r\n-print(\"=\"*60)\n+print(\"=\"*60)\r\n"
                }
            ],
            "date": 1768335944945,
            "name": "Commit-0",
            "content": "import numpy as np\r\nimport sys\r\nsys.path.append('..')\r\n\r\nfrom utils.data_utils import load_data, split_data, normalize_data\r\n\r\nfrom core.network import NeuralNetwork\r\nfrom layers.dense import DenseLayer\r\nfrom layers.activations import ReLU, Tanh\r\nfrom layers.normalization import Dropout, BatchNorm\r\nfrom losses.loss_functions import SoftmaxCrossEntropy\r\nfrom optimizers.optimizer_classes import Adam, Momentum\r\nfrom training.trainer import Trainer\r\n\r\nX, y = load_data('digits')\r\n\r\nX, mean, std = normalize_data(X)\r\n\r\nX_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\r\n\r\nprint(\"Building advanced network with regularization...\")\r\nnetwork = NeuralNetwork()\r\nnetwork.add_layer(DenseLayer(64, 256))\r\nnetwork.add_layer(BatchNorm(256))    # BatchNorm بعد Dense مباشرة\r\nnetwork.add_layer(ReLU())            # Activation بعد BatchNorm\r\nnetwork.add_layer(Dropout(0.3))\r\n\r\nnetwork.add_layer(DenseLayer(256, 128))\r\nnetwork.add_layer(Tanh())\r\nnetwork.add_layer(BatchNorm(128))\r\nnetwork.add_layer(Dropout(0.3))\r\n\r\nnetwork.add_layer(DenseLayer(128, 64))\r\nnetwork.add_layer(ReLU())\r\nnetwork.add_layer(Dropout(0.2))\r\n\r\nnetwork.add_layer(DenseLayer(64, 10))\r\nnetwork.set_loss(SoftmaxCrossEntropy())\r\n\r\noptimizer = Adam(lr=0.001, beta1=0.9, beta2=0.999)\r\ntrainer = Trainer(network, optimizer)\r\n\r\nprint(\"Training advanced network on Digits dataset...\")\r\ntrainer.fit(X_train, y_train, X_test, y_test, epochs=100, batch_size=64, verbose=True)\r\n\r\nprint(\"\" + \"=\"*60)\r\nprint(f\"Final Training Accuracy: {network.accuracy(X_train, y_train):.4f}\")\r\nprint(f\"Final Test Accuracy: {network.accuracy(X_test, y_test):.4f}\")\r\nprint(\"=\"*60)"
        }
    ]
}