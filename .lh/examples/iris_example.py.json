{
    "sourceFile": "examples/iris_example.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 12,
            "patches": [
                {
                    "date": 1768336029163,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1768337606893,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,38 +1,82 @@\n+\r\n import numpy as np\r\n import sys\r\n sys.path.append('..')\r\n \r\n from utils.data_utils import load_data, split_data, normalize_data\r\n-\r\n from core.network import NeuralNetwork\r\n from layers.dense import DenseLayer\r\n-from layers.activations import ReLU, Sigmoid\r\n-from layers.normalization import BatchNorm\r\n+from layers.activations import ReLU\r\n+from layers.normalization import Dropout\r\n from losses.loss_functions import SoftmaxCrossEntropy\r\n from optimizers.optimizer_classes import Adam\r\n from training.trainer import Trainer\r\n \r\n+# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\r\n X, y = load_data('iris')\r\n-\r\n X, mean, std = normalize_data(X)\r\n-\r\n X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\r\n \r\n-print(\"Building network architecture...\")\r\n+print(\"=\" * 70)\r\n+print(\"ğŸŒ¸ Iris Classification - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©\")\r\n+print(\"=\" * 70)\r\n+\r\n+# Ø¨Ù†Ø§Ø¡ Ø´Ø¨ÙƒØ© Ø¨Ø³ÙŠØ·Ø© ÙˆÙØ¹Ø§Ù„Ø©\r\n network = NeuralNetwork()\r\n-network.add_layer(DenseLayer(4, 16))\r\n-network.add_layer(BatchNorm(16))\r\n-network.add_layer(Sigmoid())\r\n-network.add_layer(DenseLayer(16, 8))\r\n+network.add_layer(DenseLayer(4, 8, weight_init='he'))     # Ø£ØµØºØ± Ù…Ù† 16\r\n network.add_layer(ReLU())\r\n-network.add_layer(DenseLayer(8, 3))\r\n+network.add_layer(Dropout(0.2))                           # regularization\r\n+network.add_layer(DenseLayer(8, 3, weight_init='he'))\r\n network.set_loss(SoftmaxCrossEntropy())\r\n \r\n-optimizer = Adam(lr=0.01)\r\n-trainer = Trainer(network, optimizer)\r\n+# Optimizer Ù…Ø¹ L2 regularization\r\n+optimizer = Adam(lr=0.005, beta1=0.9, beta2=0.999)\r\n+trainer = Trainer(network, optimizer, weight_decay=0.001)  # L2 regularization\r\n \r\n-print(\"Training on Iris dataset...\")\r\n-trainer.fit(X_train, y_train, X_test, y_test, epochs=100, batch_size=16, verbose=True)\r\n+print(\"\\nğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Early Stopping Ùˆ LR Decay...\")\r\n+print(\"-\" * 70)\r\n \r\n-print(f\"Final Training Accuracy: {network.accuracy(X_train, y_train):.4f}\")\r\n-print(f\"Final Test Accuracy: {network.accuracy(X_test, y_test):.4f}\")\r\n+# ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª\r\n+trainer.fit(\r\n+    X_train, y_train, \r\n+    X_test, y_test,\r\n+    epochs=150,\r\n+    batch_size=16,\r\n+    verbose=True,\r\n+    early_stopping=True,   # ØªÙØ¹ÙŠÙ„ early stopping\r\n+    patience=20,           # Ø§Ù†ØªØ¸Ø§Ø± 20 epoch\r\n+    lr_decay=True,         # ØªÙØ¹ÙŠÙ„ LR decay\r\n+    decay_rate=0.95,\r\n+    decay_every=30\r\n+)\r\n+\r\n+print(\"\\n\" + \"=\" * 70)\r\n+print(\"ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\")\r\n+print(\"=\" * 70)\r\n+\r\n+train_acc = network.accuracy(X_train, y_train)\r\n+test_acc = network.accuracy(X_test, y_test)\r\n+gap = train_acc - test_acc\r\n+\r\n+print(f\"Training Accuracy:   {train_acc:.4f}\")\r\n+print(f\"Test Accuracy:       {test_acc:.4f}\")\r\n+print(f\"Overfitting Gap:     {gap:.4f}\")\r\n+\r\n+if gap < 0.03:\r\n+    print(\"\\nâœ… Ù…Ù…ØªØ§Ø²! Ù„Ø§ ÙŠÙˆØ¬Ø¯ overfitting\")\r\n+elif gap < 0.05:\r\n+    print(\"\\nâœ“ Ø¬ÙŠØ¯! overfitting Ù‚Ù„ÙŠÙ„\")\r\n+else:\r\n+    print(\"\\nâš ï¸ ÙŠÙˆØ¬Ø¯ overfittingØŒ Ø­Ø§ÙˆÙ„ ØªØ²ÙŠØ¯ dropout Ø£Ùˆ weight_decay\")\r\n+\r\n+print(\"=\" * 70)\r\n+\r\n+print(\"\\nğŸ’¡ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:\")\r\n+print(\"-\" * 70)\r\n+print(\"âœ“ Simplified Architecture (8 units Ø¨Ø¯Ù„ 16)\")\r\n+print(\"âœ“ Dropout (0.2) Ù„Ù„Ù€ regularization\")\r\n+print(\"âœ“ L2 Regularization (weight_decay=0.001)\")\r\n+print(\"âœ“ Early Stopping (patience=20)\")\r\n+print(\"âœ“ Learning Rate Decay (0.95 every 30 epochs)\")\r\n+print(\"âœ“ He Initialization Ù„Ù„Ù€ weights\")\r\n+print(\"=\" * 70)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1768337838614,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,82 @@\n+\r\n+import numpy as np\r\n+import sys\r\n+sys.path.append('..')\r\n+\r\n+from utils.data_utils import load_data, split_data, normalize_data\r\n+from core.network import NeuralNetwork\r\n+from layers.dense import DenseLayer\r\n+from layers.activations import ReLU\r\n+from layers.normalization import Dropout\r\n+from losses.loss_functions import SoftmaxCrossEntropy\r\n+from optimizers.optimizer_classes import Adam\r\n+from training.trainer import Trainer\r\n+\r\n+# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\r\n+X, y = load_data('iris')\r\n+X, mean, std = normalize_data(X)\r\n+X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\r\n+\r\n+print(\"=\" * 70)\r\n+print(\"ğŸŒ¸ Iris Classification - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©\")\r\n+print(\"=\" * 70)\r\n+\r\n+# Ø¨Ù†Ø§Ø¡ Ø´Ø¨ÙƒØ© Ø¨Ø³ÙŠØ·Ø© ÙˆÙØ¹Ø§Ù„Ø©\r\n+network = NeuralNetwork()\r\n+network.add_layer(DenseLayer(4, 8, weight_init='he'))     # Ø£ØµØºØ± Ù…Ù† 16\r\n+network.add_layer(ReLU())\r\n+network.add_layer(Dropout(0.))                           # regularization\r\n+network.add_layer(DenseLayer(8, 3, weight_init='he'))\r\n+network.set_loss(SoftmaxCrossEntropy())\r\n+\r\n+# Optimizer Ù…Ø¹ L2 regularization\r\n+optimizer = Adam(lr=0.005, beta1=0.9, beta2=0.999)\r\n+trainer = Trainer(network, optimizer, weight_decay=0.001)  # L2 regularization\r\n+\r\n+print(\"\\nğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Early Stopping Ùˆ LR Decay...\")\r\n+print(\"-\" * 70)\r\n+\r\n+# ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª\r\n+trainer.fit(\r\n+    X_train, y_train, \r\n+    X_test, y_test,\r\n+    epochs=150,\r\n+    batch_size=16,\r\n+    verbose=True,\r\n+    early_stopping=True,   # ØªÙØ¹ÙŠÙ„ early stopping\r\n+    patience=20,           # Ø§Ù†ØªØ¸Ø§Ø± 20 epoch\r\n+    lr_decay=True,         # ØªÙØ¹ÙŠÙ„ LR decay\r\n+    decay_rate=0.95,\r\n+    decay_every=30\r\n+)\r\n+\r\n+print(\"\\n\" + \"=\" * 70)\r\n+print(\"ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\")\r\n+print(\"=\" * 70)\r\n+\r\n+train_acc = network.accuracy(X_train, y_train)\r\n+test_acc = network.accuracy(X_test, y_test)\r\n+gap = train_acc - test_acc\r\n+\r\n+print(f\"Training Accuracy:   {train_acc:.4f}\")\r\n+print(f\"Test Accuracy:       {test_acc:.4f}\")\r\n+print(f\"Overfitting Gap:     {gap:.4f}\")\r\n+\r\n+if gap < 0.03:\r\n+    print(\"\\nâœ… Ù…Ù…ØªØ§Ø²! Ù„Ø§ ÙŠÙˆØ¬Ø¯ overfitting\")\r\n+elif gap < 0.05:\r\n+    print(\"\\nâœ“ Ø¬ÙŠØ¯! overfitting Ù‚Ù„ÙŠÙ„\")\r\n+else:\r\n+    print(\"\\nâš ï¸ ÙŠÙˆØ¬Ø¯ overfittingØŒ Ø­Ø§ÙˆÙ„ ØªØ²ÙŠØ¯ dropout Ø£Ùˆ weight_decay\")\r\n+\r\n+print(\"=\" * 70)\r\n+\r\n+print(\"\\nğŸ’¡ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:\")\r\n+print(\"-\" * 70)\r\n+print(\"âœ“ Simplified Architecture (8 units Ø¨Ø¯Ù„ 16)\")\r\n+print(\"âœ“ Dropout (0.2) Ù„Ù„Ù€ regularization\")\r\n+print(\"âœ“ L2 Regularization (weight_decay=0.001)\")\r\n+print(\"âœ“ Early Stopping (patience=20)\")\r\n+print(\"âœ“ Learning Rate Decay (0.95 every 30 epochs)\")\r\n+print(\"âœ“ He Initialization Ù„Ù„Ù€ weights\")\r\n+print(\"=\" * 70)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1768337929870,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,14 +24,14 @@\n # Ø¨Ù†Ø§Ø¡ Ø´Ø¨ÙƒØ© Ø¨Ø³ÙŠØ·Ø© ÙˆÙØ¹Ø§Ù„Ø©\r\n network = NeuralNetwork()\r\n network.add_layer(DenseLayer(4, 8, weight_init='he'))     # Ø£ØµØºØ± Ù…Ù† 16\r\n network.add_layer(ReLU())\r\n-network.add_layer(Dropout(0.))                           # regularization\r\n+network.add_layer(Dropout(0.3))                           # regularization\r\n network.add_layer(DenseLayer(8, 3, weight_init='he'))\r\n network.set_loss(SoftmaxCrossEntropy())\r\n \r\n # Optimizer Ù…Ø¹ L2 regularization\r\n-optimizer = Adam(lr=0.005, beta1=0.9, beta2=0.999)\r\n+optimizer = Adam(lr=0.003, beta1=0.9, beta2=0.999)\r\n trainer = Trainer(network, optimizer, weight_decay=0.001)  # L2 regularization\r\n \r\n print(\"\\nğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Early Stopping Ùˆ LR Decay...\")\r\n print(\"-\" * 70)\r\n@@ -78,87 +78,5 @@\n print(\"âœ“ L2 Regularization (weight_decay=0.001)\")\r\n print(\"âœ“ Early Stopping (patience=20)\")\r\n print(\"âœ“ Learning Rate Decay (0.95 every 30 epochs)\")\r\n print(\"âœ“ He Initialization Ù„Ù„Ù€ weights\")\r\n-print(\"=\" * 70)\n-\r\n-import numpy as np\r\n-import sys\r\n-sys.path.append('..')\r\n-\r\n-from utils.data_utils import load_data, split_data, normalize_data\r\n-from core.network import NeuralNetwork\r\n-from layers.dense import DenseLayer\r\n-from layers.activations import ReLU\r\n-from layers.normalization import Dropout\r\n-from losses.loss_functions import SoftmaxCrossEntropy\r\n-from optimizers.optimizer_classes import Adam\r\n-from training.trainer import Trainer\r\n-\r\n-# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\r\n-X, y = load_data('iris')\r\n-X, mean, std = normalize_data(X)\r\n-X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\r\n-\r\n-print(\"=\" * 70)\r\n-print(\"ğŸŒ¸ Iris Classification - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©\")\r\n-print(\"=\" * 70)\r\n-\r\n-# Ø¨Ù†Ø§Ø¡ Ø´Ø¨ÙƒØ© Ø¨Ø³ÙŠØ·Ø© ÙˆÙØ¹Ø§Ù„Ø©\r\n-network = NeuralNetwork()\r\n-network.add_layer(DenseLayer(4, 8, weight_init='he'))     # Ø£ØµØºØ± Ù…Ù† 16\r\n-network.add_layer(ReLU())\r\n-network.add_layer(Dropout(0.2))                           # regularization\r\n-network.add_layer(DenseLayer(8, 3, weight_init='he'))\r\n-network.set_loss(SoftmaxCrossEntropy())\r\n-\r\n-# Optimizer Ù…Ø¹ L2 regularization\r\n-optimizer = Adam(lr=0.005, beta1=0.9, beta2=0.999)\r\n-trainer = Trainer(network, optimizer, weight_decay=0.001)  # L2 regularization\r\n-\r\n-print(\"\\nğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Early Stopping Ùˆ LR Decay...\")\r\n-print(\"-\" * 70)\r\n-\r\n-# ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª\r\n-trainer.fit(\r\n-    X_train, y_train, \r\n-    X_test, y_test,\r\n-    epochs=150,\r\n-    batch_size=16,\r\n-    verbose=True,\r\n-    early_stopping=True,   # ØªÙØ¹ÙŠÙ„ early stopping\r\n-    patience=20,           # Ø§Ù†ØªØ¸Ø§Ø± 20 epoch\r\n-    lr_decay=True,         # ØªÙØ¹ÙŠÙ„ LR decay\r\n-    decay_rate=0.95,\r\n-    decay_every=30\r\n-)\r\n-\r\n-print(\"\\n\" + \"=\" * 70)\r\n-print(\"ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\")\r\n-print(\"=\" * 70)\r\n-\r\n-train_acc = network.accuracy(X_train, y_train)\r\n-test_acc = network.accuracy(X_test, y_test)\r\n-gap = train_acc - test_acc\r\n-\r\n-print(f\"Training Accuracy:   {train_acc:.4f}\")\r\n-print(f\"Test Accuracy:       {test_acc:.4f}\")\r\n-print(f\"Overfitting Gap:     {gap:.4f}\")\r\n-\r\n-if gap < 0.03:\r\n-    print(\"\\nâœ… Ù…Ù…ØªØ§Ø²! Ù„Ø§ ÙŠÙˆØ¬Ø¯ overfitting\")\r\n-elif gap < 0.05:\r\n-    print(\"\\nâœ“ Ø¬ÙŠØ¯! overfitting Ù‚Ù„ÙŠÙ„\")\r\n-else:\r\n-    print(\"\\nâš ï¸ ÙŠÙˆØ¬Ø¯ overfittingØŒ Ø­Ø§ÙˆÙ„ ØªØ²ÙŠØ¯ dropout Ø£Ùˆ weight_decay\")\r\n-\r\n-print(\"=\" * 70)\r\n-\r\n-print(\"\\nğŸ’¡ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:\")\r\n-print(\"-\" * 70)\r\n-print(\"âœ“ Simplified Architecture (8 units Ø¨Ø¯Ù„ 16)\")\r\n-print(\"âœ“ Dropout (0.2) Ù„Ù„Ù€ regularization\")\r\n-print(\"âœ“ L2 Regularization (weight_decay=0.001)\")\r\n-print(\"âœ“ Early Stopping (patience=20)\")\r\n-print(\"âœ“ Learning Rate Decay (0.95 every 30 epochs)\")\r\n-print(\"âœ“ He Initialization Ù„Ù„Ù€ weights\")\r\n print(\"=\" * 70)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1768338256794,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,9 +11,8 @@\n from losses.loss_functions import SoftmaxCrossEntropy\r\n from optimizers.optimizer_classes import Adam\r\n from training.trainer import Trainer\r\n \r\n-# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\r\n X, y = load_data('iris')\r\n X, mean, std = normalize_data(X)\r\n X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\r\n \r\n"
                },
                {
                    "date": 1768338262126,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,81 @@\n+\r\n+import numpy as np\r\n+import sys\r\n+sys.path.append('..')\r\n+\r\n+from utils.data_utils import load_data, split_data, normalize_data\r\n+from core.network import NeuralNetwork\r\n+from layers.dense import DenseLayer\r\n+from layers.activations import ReLU\r\n+from layers.normalization import Dropout\r\n+from losses.loss_functions import SoftmaxCrossEntropy\r\n+from optimizers.optimizer_classes import Adam\r\n+from training.trainer import Trainer\r\n+\r\n+X, y = load_data('iris')\r\n+X, mean, std = normalize_data(X)\r\n+X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\r\n+\r\n+print(\"=\" * 70)\r\n+print(\"Iris Classification - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©\")\r\n+print(\"=\" * 70)\r\n+\r\n+# Ø¨Ù†Ø§Ø¡ Ø´Ø¨ÙƒØ© Ø¨Ø³ÙŠØ·Ø© ÙˆÙØ¹Ø§Ù„Ø©\r\n+network = NeuralNetwork()\r\n+network.add_layer(DenseLayer(4, 8, weight_init='he'))     # Ø£ØµØºØ± Ù…Ù† 16\r\n+network.add_layer(ReLU())\r\n+network.add_layer(Dropout(0.3))                           # regularization\r\n+network.add_layer(DenseLayer(8, 3, weight_init='he'))\r\n+network.set_loss(SoftmaxCrossEntropy())\r\n+\r\n+# Optimizer Ù…Ø¹ L2 regularization\r\n+optimizer = Adam(lr=0.003, beta1=0.9, beta2=0.999)\r\n+trainer = Trainer(network, optimizer, weight_decay=0.001)  # L2 regularization\r\n+\r\n+print(\"\\nğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Early Stopping Ùˆ LR Decay...\")\r\n+print(\"-\" * 70)\r\n+\r\n+# ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª\r\n+trainer.fit(\r\n+    X_train, y_train, \r\n+    X_test, y_test,\r\n+    epochs=150,\r\n+    batch_size=16,\r\n+    verbose=True,\r\n+    early_stopping=True,   # ØªÙØ¹ÙŠÙ„ early stopping\r\n+    patience=20,           # Ø§Ù†ØªØ¸Ø§Ø± 20 epoch\r\n+    lr_decay=True,         # ØªÙØ¹ÙŠÙ„ LR decay\r\n+    decay_rate=0.95,\r\n+    decay_every=30\r\n+)\r\n+\r\n+print(\"\\n\" + \"=\" * 70)\r\n+print(\"ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\")\r\n+print(\"=\" * 70)\r\n+\r\n+train_acc = network.accuracy(X_train, y_train)\r\n+test_acc = network.accuracy(X_test, y_test)\r\n+gap = train_acc - test_acc\r\n+\r\n+print(f\"Training Accuracy:   {train_acc:.4f}\")\r\n+print(f\"Test Accuracy:       {test_acc:.4f}\")\r\n+print(f\"Overfitting Gap:     {gap:.4f}\")\r\n+\r\n+if gap < 0.03:\r\n+    print(\"\\nâœ… Ù…Ù…ØªØ§Ø²! Ù„Ø§ ÙŠÙˆØ¬Ø¯ overfitting\")\r\n+elif gap < 0.05:\r\n+    print(\"\\nâœ“ Ø¬ÙŠØ¯! overfitting Ù‚Ù„ÙŠÙ„\")\r\n+else:\r\n+    print(\"\\nâš ï¸ ÙŠÙˆØ¬Ø¯ overfittingØŒ Ø­Ø§ÙˆÙ„ ØªØ²ÙŠØ¯ dropout Ø£Ùˆ weight_decay\")\r\n+\r\n+print(\"=\" * 70)\r\n+\r\n+print(\"\\nğŸ’¡ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:\")\r\n+print(\"-\" * 70)\r\n+print(\"âœ“ Simplified Architecture (8 units Ø¨Ø¯Ù„ 16)\")\r\n+print(\"âœ“ Dropout (0.2) Ù„Ù„Ù€ regularization\")\r\n+print(\"âœ“ L2 Regularization (weight_decay=0.001)\")\r\n+print(\"âœ“ Early Stopping (patience=20)\")\r\n+print(\"âœ“ Learning Rate Decay (0.95 every 30 epochs)\")\r\n+print(\"âœ“ He Initialization Ù„Ù„Ù€ weights\")\r\n+print(\"=\" * 70)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1768338269129,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,12 +16,11 @@\n X, mean, std = normalize_data(X)\r\n X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\r\n \r\n print(\"=\" * 70)\r\n-print(\"Iris Classification - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©\")\r\n+print(\"Iris Classification\")\r\n print(\"=\" * 70)\r\n \r\n-# Ø¨Ù†Ø§Ø¡ Ø´Ø¨ÙƒØ© Ø¨Ø³ÙŠØ·Ø© ÙˆÙØ¹Ø§Ù„Ø©\r\n network = NeuralNetwork()\r\n network.add_layer(DenseLayer(4, 8, weight_init='he'))     # Ø£ØµØºØ± Ù…Ù† 16\r\n network.add_layer(ReLU())\r\n network.add_layer(Dropout(0.3))                           # regularization\r\n@@ -77,86 +76,5 @@\n print(\"âœ“ L2 Regularization (weight_decay=0.001)\")\r\n print(\"âœ“ Early Stopping (patience=20)\")\r\n print(\"âœ“ Learning Rate Decay (0.95 every 30 epochs)\")\r\n print(\"âœ“ He Initialization Ù„Ù„Ù€ weights\")\r\n-print(\"=\" * 70)\n-\r\n-import numpy as np\r\n-import sys\r\n-sys.path.append('..')\r\n-\r\n-from utils.data_utils import load_data, split_data, normalize_data\r\n-from core.network import NeuralNetwork\r\n-from layers.dense import DenseLayer\r\n-from layers.activations import ReLU\r\n-from layers.normalization import Dropout\r\n-from losses.loss_functions import SoftmaxCrossEntropy\r\n-from optimizers.optimizer_classes import Adam\r\n-from training.trainer import Trainer\r\n-\r\n-X, y = load_data('iris')\r\n-X, mean, std = normalize_data(X)\r\n-X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\r\n-\r\n-print(\"=\" * 70)\r\n-print(\"ğŸŒ¸ Iris Classification - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©\")\r\n-print(\"=\" * 70)\r\n-\r\n-# Ø¨Ù†Ø§Ø¡ Ø´Ø¨ÙƒØ© Ø¨Ø³ÙŠØ·Ø© ÙˆÙØ¹Ø§Ù„Ø©\r\n-network = NeuralNetwork()\r\n-network.add_layer(DenseLayer(4, 8, weight_init='he'))     # Ø£ØµØºØ± Ù…Ù† 16\r\n-network.add_layer(ReLU())\r\n-network.add_layer(Dropout(0.3))                           # regularization\r\n-network.add_layer(DenseLayer(8, 3, weight_init='he'))\r\n-network.set_loss(SoftmaxCrossEntropy())\r\n-\r\n-# Optimizer Ù…Ø¹ L2 regularization\r\n-optimizer = Adam(lr=0.003, beta1=0.9, beta2=0.999)\r\n-trainer = Trainer(network, optimizer, weight_decay=0.001)  # L2 regularization\r\n-\r\n-print(\"\\nğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Early Stopping Ùˆ LR Decay...\")\r\n-print(\"-\" * 70)\r\n-\r\n-# ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª\r\n-trainer.fit(\r\n-    X_train, y_train, \r\n-    X_test, y_test,\r\n-    epochs=150,\r\n-    batch_size=16,\r\n-    verbose=True,\r\n-    early_stopping=True,   # ØªÙØ¹ÙŠÙ„ early stopping\r\n-    patience=20,           # Ø§Ù†ØªØ¸Ø§Ø± 20 epoch\r\n-    lr_decay=True,         # ØªÙØ¹ÙŠÙ„ LR decay\r\n-    decay_rate=0.95,\r\n-    decay_every=30\r\n-)\r\n-\r\n-print(\"\\n\" + \"=\" * 70)\r\n-print(\"ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\")\r\n-print(\"=\" * 70)\r\n-\r\n-train_acc = network.accuracy(X_train, y_train)\r\n-test_acc = network.accuracy(X_test, y_test)\r\n-gap = train_acc - test_acc\r\n-\r\n-print(f\"Training Accuracy:   {train_acc:.4f}\")\r\n-print(f\"Test Accuracy:       {test_acc:.4f}\")\r\n-print(f\"Overfitting Gap:     {gap:.4f}\")\r\n-\r\n-if gap < 0.03:\r\n-    print(\"\\nâœ… Ù…Ù…ØªØ§Ø²! Ù„Ø§ ÙŠÙˆØ¬Ø¯ overfitting\")\r\n-elif gap < 0.05:\r\n-    print(\"\\nâœ“ Ø¬ÙŠØ¯! overfitting Ù‚Ù„ÙŠÙ„\")\r\n-else:\r\n-    print(\"\\nâš ï¸ ÙŠÙˆØ¬Ø¯ overfittingØŒ Ø­Ø§ÙˆÙ„ ØªØ²ÙŠØ¯ dropout Ø£Ùˆ weight_decay\")\r\n-\r\n-print(\"=\" * 70)\r\n-\r\n-print(\"\\nğŸ’¡ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:\")\r\n-print(\"-\" * 70)\r\n-print(\"âœ“ Simplified Architecture (8 units Ø¨Ø¯Ù„ 16)\")\r\n-print(\"âœ“ Dropout (0.2) Ù„Ù„Ù€ regularization\")\r\n-print(\"âœ“ L2 Regularization (weight_decay=0.001)\")\r\n-print(\"âœ“ Early Stopping (patience=20)\")\r\n-print(\"âœ“ Learning Rate Decay (0.95 every 30 epochs)\")\r\n-print(\"âœ“ He Initialization Ù„Ù„Ù€ weights\")\r\n print(\"=\" * 70)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1768338278432,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,9 +20,9 @@\n print(\"Iris Classification\")\r\n print(\"=\" * 70)\r\n \r\n network = NeuralNetwork()\r\n-network.add_layer(DenseLayer(4, 8, weight_init='he'))     # Ø£ØµØºØ± Ù…Ù† 16\r\n+network.add_layer(DenseLayer(4, 8, weight_init='he'))     \r\n network.add_layer(ReLU())\r\n network.add_layer(Dropout(0.3))                           # regularization\r\n network.add_layer(DenseLayer(8, 3, weight_init='he'))\r\n network.set_loss(SoftmaxCrossEntropy())\r\n"
                },
                {
                    "date": 1768338286273,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,79 @@\n+\r\n+import numpy as np\r\n+import sys\r\n+sys.path.append('..')\r\n+\r\n+from utils.data_utils import load_data, split_data, normalize_data\r\n+from core.network import NeuralNetwork\r\n+from layers.dense import DenseLayer\r\n+from layers.activations import ReLU\r\n+from layers.normalization import Dropout\r\n+from losses.loss_functions import SoftmaxCrossEntropy\r\n+from optimizers.optimizer_classes import Adam\r\n+from training.trainer import Trainer\r\n+\r\n+X, y = load_data('iris')\r\n+X, mean, std = normalize_data(X)\r\n+X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\r\n+\r\n+print(\"=\" * 70)\r\n+print(\"Iris Classification\")\r\n+print(\"=\" * 70)\r\n+\r\n+network = NeuralNetwork()\r\n+network.add_layer(DenseLayer(4, 8, weight_init='he'))     \r\n+network.add_layer(ReLU())\r\n+network.add_layer(Dropout(0.3))                           \r\n+network.add_layer(DenseLayer(8, 3, weight_init='he'))\r\n+network.set_loss(SoftmaxCrossEntropy())\r\n+\r\n+optimizer = Adam(lr=0.003, beta1=0.9, beta2=0.999)\r\n+trainer = Trainer(network, optimizer, weight_decay=0.001)  # L2 regularization\r\n+\r\n+print(\"\\nğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Early Stopping Ùˆ LR Decay...\")\r\n+print(\"-\" * 70)\r\n+\r\n+# ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª\r\n+trainer.fit(\r\n+    X_train, y_train, \r\n+    X_test, y_test,\r\n+    epochs=150,\r\n+    batch_size=16,\r\n+    verbose=True,\r\n+    early_stopping=True,   # ØªÙØ¹ÙŠÙ„ early stopping\r\n+    patience=20,           # Ø§Ù†ØªØ¸Ø§Ø± 20 epoch\r\n+    lr_decay=True,         # ØªÙØ¹ÙŠÙ„ LR decay\r\n+    decay_rate=0.95,\r\n+    decay_every=30\r\n+)\r\n+\r\n+print(\"\\n\" + \"=\" * 70)\r\n+print(\"ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\")\r\n+print(\"=\" * 70)\r\n+\r\n+train_acc = network.accuracy(X_train, y_train)\r\n+test_acc = network.accuracy(X_test, y_test)\r\n+gap = train_acc - test_acc\r\n+\r\n+print(f\"Training Accuracy:   {train_acc:.4f}\")\r\n+print(f\"Test Accuracy:       {test_acc:.4f}\")\r\n+print(f\"Overfitting Gap:     {gap:.4f}\")\r\n+\r\n+if gap < 0.03:\r\n+    print(\"\\nâœ… Ù…Ù…ØªØ§Ø²! Ù„Ø§ ÙŠÙˆØ¬Ø¯ overfitting\")\r\n+elif gap < 0.05:\r\n+    print(\"\\nâœ“ Ø¬ÙŠØ¯! overfitting Ù‚Ù„ÙŠÙ„\")\r\n+else:\r\n+    print(\"\\nâš ï¸ ÙŠÙˆØ¬Ø¯ overfittingØŒ Ø­Ø§ÙˆÙ„ ØªØ²ÙŠØ¯ dropout Ø£Ùˆ weight_decay\")\r\n+\r\n+print(\"=\" * 70)\r\n+\r\n+print(\"\\nğŸ’¡ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:\")\r\n+print(\"-\" * 70)\r\n+print(\"âœ“ Simplified Architecture (8 units Ø¨Ø¯Ù„ 16)\")\r\n+print(\"âœ“ Dropout (0.2) Ù„Ù„Ù€ regularization\")\r\n+print(\"âœ“ L2 Regularization (weight_decay=0.001)\")\r\n+print(\"âœ“ Early Stopping (patience=20)\")\r\n+print(\"âœ“ Learning Rate Decay (0.95 every 30 epochs)\")\r\n+print(\"âœ“ He Initialization Ù„Ù„Ù€ weights\")\r\n+print(\"=\" * 70)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1768338292944,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,79 @@\n+\r\n+import numpy as np\r\n+import sys\r\n+sys.path.append('..')\r\n+\r\n+from utils.data_utils import load_data, split_data, normalize_data\r\n+from core.network import NeuralNetwork\r\n+from layers.dense import DenseLayer\r\n+from layers.activations import ReLU\r\n+from layers.normalization import Dropout\r\n+from losses.loss_functions import SoftmaxCrossEntropy\r\n+from optimizers.optimizer_classes import Adam\r\n+from training.trainer import Trainer\r\n+\r\n+X, y = load_data('iris')\r\n+X, mean, std = normalize_data(X)\r\n+X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\r\n+\r\n+print(\"=\" * 70)\r\n+print(\"Iris Classification\")\r\n+print(\"=\" * 70)\r\n+\r\n+network = NeuralNetwork()\r\n+network.add_layer(DenseLayer(4, 8, weight_init='he'))     \r\n+network.add_layer(ReLU())\r\n+network.add_layer(Dropout(0.3))                           \r\n+network.add_layer(DenseLayer(8, 3, weight_init='he'))\r\n+network.set_loss(SoftmaxCrossEntropy())\r\n+\r\n+optimizer = Adam(lr=0.003, beta1=0.9, beta2=0.999)\r\n+trainer = Trainer(network, optimizer, weight_decay=0.001) \r\n+\r\n+print(\"\\nğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Early Stopping Ùˆ LR Decay...\")\r\n+print(\"-\" * 70)\r\n+\r\n+# ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª\r\n+trainer.fit(\r\n+    X_train, y_train, \r\n+    X_test, y_test,\r\n+    epochs=150,\r\n+    batch_size=16,\r\n+    verbose=True,\r\n+    early_stopping=True,   # ØªÙØ¹ÙŠÙ„ early stopping\r\n+    patience=20,           # Ø§Ù†ØªØ¸Ø§Ø± 20 epoch\r\n+    lr_decay=True,         # ØªÙØ¹ÙŠÙ„ LR decay\r\n+    decay_rate=0.95,\r\n+    decay_every=30\r\n+)\r\n+\r\n+print(\"\\n\" + \"=\" * 70)\r\n+print(\"ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\")\r\n+print(\"=\" * 70)\r\n+\r\n+train_acc = network.accuracy(X_train, y_train)\r\n+test_acc = network.accuracy(X_test, y_test)\r\n+gap = train_acc - test_acc\r\n+\r\n+print(f\"Training Accuracy:   {train_acc:.4f}\")\r\n+print(f\"Test Accuracy:       {test_acc:.4f}\")\r\n+print(f\"Overfitting Gap:     {gap:.4f}\")\r\n+\r\n+if gap < 0.03:\r\n+    print(\"\\nâœ… Ù…Ù…ØªØ§Ø²! Ù„Ø§ ÙŠÙˆØ¬Ø¯ overfitting\")\r\n+elif gap < 0.05:\r\n+    print(\"\\nâœ“ Ø¬ÙŠØ¯! overfitting Ù‚Ù„ÙŠÙ„\")\r\n+else:\r\n+    print(\"\\nâš ï¸ ÙŠÙˆØ¬Ø¯ overfittingØŒ Ø­Ø§ÙˆÙ„ ØªØ²ÙŠØ¯ dropout Ø£Ùˆ weight_decay\")\r\n+\r\n+print(\"=\" * 70)\r\n+\r\n+print(\"\\nğŸ’¡ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:\")\r\n+print(\"-\" * 70)\r\n+print(\"âœ“ Simplified Architecture (8 units Ø¨Ø¯Ù„ 16)\")\r\n+print(\"âœ“ Dropout (0.2) Ù„Ù„Ù€ regularization\")\r\n+print(\"âœ“ L2 Regularization (weight_decay=0.001)\")\r\n+print(\"âœ“ Early Stopping (patience=20)\")\r\n+print(\"âœ“ Learning Rate Decay (0.95 every 30 epochs)\")\r\n+print(\"âœ“ He Initialization Ù„Ù„Ù€ weights\")\r\n+print(\"=\" * 70)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1768338301000,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -75,164 +75,5 @@\n print(\"âœ“ L2 Regularization (weight_decay=0.001)\")\r\n print(\"âœ“ Early Stopping (patience=20)\")\r\n print(\"âœ“ Learning Rate Decay (0.95 every 30 epochs)\")\r\n print(\"âœ“ He Initialization Ù„Ù„Ù€ weights\")\r\n-print(\"=\" * 70)\n-\r\n-import numpy as np\r\n-import sys\r\n-sys.path.append('..')\r\n-\r\n-from utils.data_utils import load_data, split_data, normalize_data\r\n-from core.network import NeuralNetwork\r\n-from layers.dense import DenseLayer\r\n-from layers.activations import ReLU\r\n-from layers.normalization import Dropout\r\n-from losses.loss_functions import SoftmaxCrossEntropy\r\n-from optimizers.optimizer_classes import Adam\r\n-from training.trainer import Trainer\r\n-\r\n-X, y = load_data('iris')\r\n-X, mean, std = normalize_data(X)\r\n-X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\r\n-\r\n-print(\"=\" * 70)\r\n-print(\"Iris Classification\")\r\n-print(\"=\" * 70)\r\n-\r\n-network = NeuralNetwork()\r\n-network.add_layer(DenseLayer(4, 8, weight_init='he'))     \r\n-network.add_layer(ReLU())\r\n-network.add_layer(Dropout(0.3))                           \r\n-network.add_layer(DenseLayer(8, 3, weight_init='he'))\r\n-network.set_loss(SoftmaxCrossEntropy())\r\n-\r\n-optimizer = Adam(lr=0.003, beta1=0.9, beta2=0.999)\r\n-trainer = Trainer(network, optimizer, weight_decay=0.001)  # L2 regularization\r\n-\r\n-print(\"\\nğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Early Stopping Ùˆ LR Decay...\")\r\n-print(\"-\" * 70)\r\n-\r\n-# ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª\r\n-trainer.fit(\r\n-    X_train, y_train, \r\n-    X_test, y_test,\r\n-    epochs=150,\r\n-    batch_size=16,\r\n-    verbose=True,\r\n-    early_stopping=True,   # ØªÙØ¹ÙŠÙ„ early stopping\r\n-    patience=20,           # Ø§Ù†ØªØ¸Ø§Ø± 20 epoch\r\n-    lr_decay=True,         # ØªÙØ¹ÙŠÙ„ LR decay\r\n-    decay_rate=0.95,\r\n-    decay_every=30\r\n-)\r\n-\r\n-print(\"\\n\" + \"=\" * 70)\r\n-print(\"ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\")\r\n-print(\"=\" * 70)\r\n-\r\n-train_acc = network.accuracy(X_train, y_train)\r\n-test_acc = network.accuracy(X_test, y_test)\r\n-gap = train_acc - test_acc\r\n-\r\n-print(f\"Training Accuracy:   {train_acc:.4f}\")\r\n-print(f\"Test Accuracy:       {test_acc:.4f}\")\r\n-print(f\"Overfitting Gap:     {gap:.4f}\")\r\n-\r\n-if gap < 0.03:\r\n-    print(\"\\nâœ… Ù…Ù…ØªØ§Ø²! Ù„Ø§ ÙŠÙˆØ¬Ø¯ overfitting\")\r\n-elif gap < 0.05:\r\n-    print(\"\\nâœ“ Ø¬ÙŠØ¯! overfitting Ù‚Ù„ÙŠÙ„\")\r\n-else:\r\n-    print(\"\\nâš ï¸ ÙŠÙˆØ¬Ø¯ overfittingØŒ Ø­Ø§ÙˆÙ„ ØªØ²ÙŠØ¯ dropout Ø£Ùˆ weight_decay\")\r\n-\r\n-print(\"=\" * 70)\r\n-\r\n-print(\"\\nğŸ’¡ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:\")\r\n-print(\"-\" * 70)\r\n-print(\"âœ“ Simplified Architecture (8 units Ø¨Ø¯Ù„ 16)\")\r\n-print(\"âœ“ Dropout (0.2) Ù„Ù„Ù€ regularization\")\r\n-print(\"âœ“ L2 Regularization (weight_decay=0.001)\")\r\n-print(\"âœ“ Early Stopping (patience=20)\")\r\n-print(\"âœ“ Learning Rate Decay (0.95 every 30 epochs)\")\r\n-print(\"âœ“ He Initialization Ù„Ù„Ù€ weights\")\r\n-print(\"=\" * 70)\n-\r\n-import numpy as np\r\n-import sys\r\n-sys.path.append('..')\r\n-\r\n-from utils.data_utils import load_data, split_data, normalize_data\r\n-from core.network import NeuralNetwork\r\n-from layers.dense import DenseLayer\r\n-from layers.activations import ReLU\r\n-from layers.normalization import Dropout\r\n-from losses.loss_functions import SoftmaxCrossEntropy\r\n-from optimizers.optimizer_classes import Adam\r\n-from training.trainer import Trainer\r\n-\r\n-X, y = load_data('iris')\r\n-X, mean, std = normalize_data(X)\r\n-X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\r\n-\r\n-print(\"=\" * 70)\r\n-print(\"Iris Classification\")\r\n-print(\"=\" * 70)\r\n-\r\n-network = NeuralNetwork()\r\n-network.add_layer(DenseLayer(4, 8, weight_init='he'))     \r\n-network.add_layer(ReLU())\r\n-network.add_layer(Dropout(0.3))                           # regularization\r\n-network.add_layer(DenseLayer(8, 3, weight_init='he'))\r\n-network.set_loss(SoftmaxCrossEntropy())\r\n-\r\n-# Optimizer Ù…Ø¹ L2 regularization\r\n-optimizer = Adam(lr=0.003, beta1=0.9, beta2=0.999)\r\n-trainer = Trainer(network, optimizer, weight_decay=0.001)  # L2 regularization\r\n-\r\n-print(\"\\nğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Early Stopping Ùˆ LR Decay...\")\r\n-print(\"-\" * 70)\r\n-\r\n-# ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª\r\n-trainer.fit(\r\n-    X_train, y_train, \r\n-    X_test, y_test,\r\n-    epochs=150,\r\n-    batch_size=16,\r\n-    verbose=True,\r\n-    early_stopping=True,   # ØªÙØ¹ÙŠÙ„ early stopping\r\n-    patience=20,           # Ø§Ù†ØªØ¸Ø§Ø± 20 epoch\r\n-    lr_decay=True,         # ØªÙØ¹ÙŠÙ„ LR decay\r\n-    decay_rate=0.95,\r\n-    decay_every=30\r\n-)\r\n-\r\n-print(\"\\n\" + \"=\" * 70)\r\n-print(\"ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\")\r\n-print(\"=\" * 70)\r\n-\r\n-train_acc = network.accuracy(X_train, y_train)\r\n-test_acc = network.accuracy(X_test, y_test)\r\n-gap = train_acc - test_acc\r\n-\r\n-print(f\"Training Accuracy:   {train_acc:.4f}\")\r\n-print(f\"Test Accuracy:       {test_acc:.4f}\")\r\n-print(f\"Overfitting Gap:     {gap:.4f}\")\r\n-\r\n-if gap < 0.03:\r\n-    print(\"\\nâœ… Ù…Ù…ØªØ§Ø²! Ù„Ø§ ÙŠÙˆØ¬Ø¯ overfitting\")\r\n-elif gap < 0.05:\r\n-    print(\"\\nâœ“ Ø¬ÙŠØ¯! overfitting Ù‚Ù„ÙŠÙ„\")\r\n-else:\r\n-    print(\"\\nâš ï¸ ÙŠÙˆØ¬Ø¯ overfittingØŒ Ø­Ø§ÙˆÙ„ ØªØ²ÙŠØ¯ dropout Ø£Ùˆ weight_decay\")\r\n-\r\n-print(\"=\" * 70)\r\n-\r\n-print(\"\\nğŸ’¡ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:\")\r\n-print(\"-\" * 70)\r\n-print(\"âœ“ Simplified Architecture (8 units Ø¨Ø¯Ù„ 16)\")\r\n-print(\"âœ“ Dropout (0.2) Ù„Ù„Ù€ regularization\")\r\n-print(\"âœ“ L2 Regularization (weight_decay=0.001)\")\r\n-print(\"âœ“ Early Stopping (patience=20)\")\r\n-print(\"âœ“ Learning Rate Decay (0.95 every 30 epochs)\")\r\n-print(\"âœ“ He Initialization Ù„Ù„Ù€ weights\")\r\n print(\"=\" * 70)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1768338322690,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,12 +29,9 @@\n \r\n optimizer = Adam(lr=0.003, beta1=0.9, beta2=0.999)\r\n trainer = Trainer(network, optimizer, weight_decay=0.001) \r\n \r\n-print(\" Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Early Stopping Ùˆ LR Decay...\")\r\n-print(\"-\" * 70)\r\n \r\n-# ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª\r\n trainer.fit(\r\n     X_train, y_train, \r\n     X_test, y_test,\r\n     epochs=150,\r\n"
                },
                {
                    "date": 1768338329279,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,75 @@\n+\r\n+import numpy as np\r\n+import sys\r\n+sys.path.append('..')\r\n+\r\n+from utils.data_utils import load_data, split_data, normalize_data\r\n+from core.network import NeuralNetwork\r\n+from layers.dense import DenseLayer\r\n+from layers.activations import ReLU\r\n+from layers.normalization import Dropout\r\n+from losses.loss_functions import SoftmaxCrossEntropy\r\n+from optimizers.optimizer_classes import Adam\r\n+from training.trainer import Trainer\r\n+\r\n+X, y = load_data('iris')\r\n+X, mean, std = normalize_data(X)\r\n+X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\r\n+\r\n+print(\"=\" * 70)\r\n+print(\"Iris Classification\")\r\n+print(\"=\" * 70)\r\n+\r\n+network = NeuralNetwork()\r\n+network.add_layer(DenseLayer(4, 8, weight_init='he'))     \r\n+network.add_layer(ReLU())\r\n+network.add_layer(Dropout(0.3))                           \r\n+network.add_layer(DenseLayer(8, 3, weight_init='he'))\r\n+network.set_loss(SoftmaxCrossEntropy())\r\n+\r\n+optimizer = Adam(lr=0.003, beta1=0.9, beta2=0.999)\r\n+trainer = Trainer(network, optimizer, weight_decay=0.001) \r\n+\r\n+trainer.fit(\r\n+    X_train, y_train, \r\n+    X_test, y_test,\r\n+    epochs=150,\r\n+    batch_size=16,\r\n+    verbose=True,\r\n+    early_stopping=True,   \r\n+    patience=20,           # Ø§Ù†ØªØ¸Ø§Ø± 20 epoch\r\n+    lr_decay=True,         # ØªÙØ¹ÙŠÙ„ LR decay\r\n+    decay_rate=0.95,\r\n+    decay_every=30\r\n+)\r\n+\r\n+print(\"\\n\" + \"=\" * 70)\r\n+print(\"ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\")\r\n+print(\"=\" * 70)\r\n+\r\n+train_acc = network.accuracy(X_train, y_train)\r\n+test_acc = network.accuracy(X_test, y_test)\r\n+gap = train_acc - test_acc\r\n+\r\n+print(f\"Training Accuracy:   {train_acc:.4f}\")\r\n+print(f\"Test Accuracy:       {test_acc:.4f}\")\r\n+print(f\"Overfitting Gap:     {gap:.4f}\")\r\n+\r\n+if gap < 0.03:\r\n+    print(\"\\nâœ… Ù…Ù…ØªØ§Ø²! Ù„Ø§ ÙŠÙˆØ¬Ø¯ overfitting\")\r\n+elif gap < 0.05:\r\n+    print(\"\\nâœ“ Ø¬ÙŠØ¯! overfitting Ù‚Ù„ÙŠÙ„\")\r\n+else:\r\n+    print(\"\\nâš ï¸ ÙŠÙˆØ¬Ø¯ overfittingØŒ Ø­Ø§ÙˆÙ„ ØªØ²ÙŠØ¯ dropout Ø£Ùˆ weight_decay\")\r\n+\r\n+print(\"=\" * 70)\r\n+\r\n+print(\"\\nğŸ’¡ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:\")\r\n+print(\"-\" * 70)\r\n+print(\"âœ“ Simplified Architecture (8 units Ø¨Ø¯Ù„ 16)\")\r\n+print(\"âœ“ Dropout (0.2) Ù„Ù„Ù€ regularization\")\r\n+print(\"âœ“ L2 Regularization (weight_decay=0.001)\")\r\n+print(\"âœ“ Early Stopping (patience=20)\")\r\n+print(\"âœ“ Learning Rate Decay (0.95 every 30 epochs)\")\r\n+print(\"âœ“ He Initialization Ù„Ù„Ù€ weights\")\r\n+print(\"=\" * 70)\n\\ No newline at end of file\n"
                }
            ],
            "date": 1768336029163,
            "name": "Commit-0",
            "content": "import numpy as np\r\nimport sys\r\nsys.path.append('..')\r\n\r\nfrom utils.data_utils import load_data, split_data, normalize_data\r\n\r\nfrom core.network import NeuralNetwork\r\nfrom layers.dense import DenseLayer\r\nfrom layers.activations import ReLU, Sigmoid\r\nfrom layers.normalization import BatchNorm\r\nfrom losses.loss_functions import SoftmaxCrossEntropy\r\nfrom optimizers.optimizer_classes import Adam\r\nfrom training.trainer import Trainer\r\n\r\nX, y = load_data('iris')\r\n\r\nX, mean, std = normalize_data(X)\r\n\r\nX_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\r\n\r\nprint(\"Building network architecture...\")\r\nnetwork = NeuralNetwork()\r\nnetwork.add_layer(DenseLayer(4, 16))\r\nnetwork.add_layer(BatchNorm(16))\r\nnetwork.add_layer(Sigmoid())\r\nnetwork.add_layer(DenseLayer(16, 8))\r\nnetwork.add_layer(ReLU())\r\nnetwork.add_layer(DenseLayer(8, 3))\r\nnetwork.set_loss(SoftmaxCrossEntropy())\r\n\r\noptimizer = Adam(lr=0.01)\r\ntrainer = Trainer(network, optimizer)\r\n\r\nprint(\"Training on Iris dataset...\")\r\ntrainer.fit(X_train, y_train, X_test, y_test, epochs=100, batch_size=16, verbose=True)\r\n\r\nprint(f\"Final Training Accuracy: {network.accuracy(X_train, y_train):.4f}\")\r\nprint(f\"Final Test Accuracy: {network.accuracy(X_test, y_test):.4f}\")\r\n"
        }
    ]
}